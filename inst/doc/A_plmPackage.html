<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Panel data econometrics in R:</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>

<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Panel data econometrics in R:</h1>
<h3 class="subtitle">the plm package</h3>
<h4 class="author">Yves Croissant</h4>
<h4 class="author">Giovanni Millo</h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>This introduction to the <code>plm</code> package is a slightly modified version of <span class="citation">Croissant and Millo (2008)</span>, published in the <em>Journal of Statistical Software</em>.</p>
<p>Panel data econometrics is obviously one of the main fields in the statistics profession, but most of the models used are difficult to estimate with only plain <code>R</code>. <code>plm</code> is a package for <code>R</code> which intends to make the estimation of linear panel models straightforward. <code>plm</code> provides functions to estimate a wide variety of models and to make (robust) inference.</p>
</div>



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Panel data econometrics is a continuously developing field. The increasing availability of data observed on cross-sections of units (like households, firms, countries etc.) <em>and</em> over time has given rise to a number of estimation approaches exploiting this double dimensionality to cope with some of the typical problems associated with economic data, first of all that of unobserved heterogeneity.</p>
<p>Timewise observation of data from different observational units has long been common in other fields of statistics (where they are often termed <em>longitudinal</em> data). In the panel data field as well as in others, the econometric approach is nevertheless peculiar with respect to experimental contexts, as it is emphasizing model specification and testing and tackling a number of issues arising from the particular statistical problems associated with economic data.</p>
<p>Thus, while a very comprehensive software framework for (among many other features) maximum likelihood estimation of linear regression models for longitudinal data, packages <code>nlme</code> <span class="citation">(Pinheiro et al. 2007)</span> and <code>lme4</code> <span class="citation">(Bates 2007)</span>, is available in the <code>R</code> (<span class="citation"> Development Core Team (2008)</span>) environment and can be used, e.g., for estimation of random effects panel models, its use is not intuitive for a practicing econometrician, and maximum likelihood estimation is only one of the possible approaches to panel data econometrics. Moreover, economic panel data sets often happen to be <em>unbalanced</em> (i.e., they have a different number of observations between groups), which case needs some adaptation to the methods and is not compatible with those in <code>nlme</code>. Hence the need for a package doing panel data “from the econometrician’s viewpoint” and featuring at a minimum the basic techniques econometricians are used to: random and fixed effects estimation of static linear panel data models, variable coefficients models, generalized method of moments estimation of dynamic models; and the basic toolbox of specification and misspecification diagnostics.</p>
<p>Furthermore, we felt there was a need for automation of some basic data management tasks such as lagging, summing and, more in general, <code>apply</code>ing (in the <code>R</code> sense) functions to the data, which, although conceptually simple, become cumbersome and error-prone on two-dimensional data, especially in the case of unbalanced panels.</p>
<p>This paper is organized as follows: Section <a href="#linear-panel-model">linear panel model</a> presents a very short overview of the typical model taxonomy<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Section <a href="#software-approach">software approach</a> discusses the software approach used in the package. The next three sections present the functionalities of the package in more detail: data management (Section <a href="#managing-data-and-formulae">managing data and formulae</a>), estimation (Section <a href="#model-estimation">model estimation</a>) and testing (Section <a href="#tests">tests</a>), giving a short description and illustrating them with examples. Section <a href="#nlme">plm vs nlme and lme4</a> compares the approach in <code>plm</code> to that of <code>nlme</code> and <code>lme4</code>, highlighting the features of the latter two that an econometrician might find most useful. Section <a href="#conclusions">conclusion</a> concludes the paper.</p>
</div>
<div id="linear-panel-model" class="section level1">
<h1>The linear panel model</h1>
<p>The basic linear panel models used in econometrics can be described through suitable restrictions of the following general model:</p>
<p><span class="math display">\[\begin{equation*}
 y_{it}=\alpha_{it} +  \beta_{it}^\top x_{it} + u_{it}
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(i=1, ..., n\)</span> is the individual (group, country …) index, <span class="math inline">\(t=1, ..., T\)</span> is the time index and <span class="math inline">\(u_{it}\)</span> a random disturbance term of mean <span class="math inline">\(0\)</span>.</p>
<p>Of course <span class="math inline">\(u_{it}\)</span> is not estimable with <span class="math inline">\(N = n \times T\)</span> data points. A number of assumptions are usually made about the parameters, the errors and the exogeneity of the regressors, giving rise to a taxonomy of feasible models for panel data.</p>
<p>The most common one is parameter homogeneity, which means that <span class="math inline">\(\alpha_{it}=\alpha\)</span> for all <span class="math inline">\(i,t\)</span> and <span class="math inline">\(\beta_{it}=\beta\)</span> for all <span class="math inline">\(i,t\)</span>. The resulting model</p>
<p><span class="math display">\[\begin{equation*}
 y_{it}=\alpha +  \beta^\top x_{it} + u_{it}
\end{equation*}\]</span></p>
<p>is a standard linear model pooling all the data across <span class="math inline">\(i\)</span> and <span class="math inline">\(t\)</span>.</p>
<p>To model individual heterogeneity, one often assumes that the error term has two separate components, one of which is specific to the individual and doesn’t change over time<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. This is called the unobserved effects model:</p>
<p><span class="math display">\[\begin{equation}
  (\#eq:errcomp)
 y_{it}=\alpha +  \beta^\top x_{it} + \mu_i + \epsilon_{it}
\end{equation}\]</span></p>
<p>The appropriate estimation method for this model depends on the properties of the two error components. The idiosyncratic error <span class="math inline">\(\epsilon_{it}\)</span> is usually assumed well-behaved and independent of both the regressors <span class="math inline">\(x_{it}\)</span> and the individual error component <span class="math inline">\(\mu_i\)</span>. The individual component may be in turn either independent of the regressors or correlated.</p>
<p>If it is correlated, the ordinary least squares (OLS) estimator of <span class="math inline">\(\beta\)</span> would be inconsistent, so it is customary to treat the <span class="math inline">\(\mu_i\)</span> as a further set of <span class="math inline">\(n\)</span> parameters to be estimated, as if in the general model <span class="math inline">\(\alpha_{it}=\alpha_{i}\)</span> for all <span class="math inline">\(t\)</span>. This is called the fixed effects (a.k.a. <em>within</em> or <em>least squares dummy variables</em>) model, usually estimated by OLS on transformed data, and gives consistent estimates for <span class="math inline">\(\beta\)</span>.</p>
<p>If the individual-specific component <span class="math inline">\(\mu_i\)</span> is uncorrelated with the regressors, a situation which is usually termed <em>random effects</em>, the overall error <span class="math inline">\(u_{it}\)</span> also is, so the OLS estimator is consistent. Nevertheless, the common error component over individuals induces correlation across the composite error terms, making OLS estimation inefficient, so one has to resort to some form of feasible generalized least squares (GLS) estimators. This is based on the estimation of the variance of the two error components, for which there are a number of different procedures available.</p>
<p>If the individual component is missing altogether, pooled OLS is the most efficient estimator for <span class="math inline">\(\beta\)</span>. This set of assumptions is usually labelled <em>pooling</em> model, although this actually refers to the errors’ properties and the appropriate estimation method rather than the model itself. If one relaxes the usual hypotheses of well-behaved, white noise errors and allows for the idiosyncratic error <span class="math inline">\(\epsilon_{it}\)</span> to be arbitrarily heteroskedastic and serially correlated over time, a more general kind of feasible GLS is needed, called the <em>unrestricted</em> or <em>general</em> GLS. This specification can also be augmented with individual-specific error components possibly correlated with the regressors, in which case it is termed <em>fixed effects</em> GLS.</p>
<p>Another way of estimating unobserved effects models through removing time-invariant individual components is by first-differencing the data: lagging the model and subtracting, the time-invariant components (the intercept and the individual error component) are eliminated, and the model</p>
<p><span class="math display">\[\begin{equation*}
 \Delta y_{it}=  \beta^\top  \Delta x_{it} + \Delta u_{it}
\end{equation*}\]</span></p>
<p>(where <span class="math inline">\(\Delta y_{it}=y_{it}-y_{i,t-1}\)</span>, <span class="math inline">\(\Delta x_{it}=x_{it}-x_{i,t-1}\)</span> and, from @ref(eq:errcomp), <span class="math inline">\(\Delta u_{it}=u_{it}-u_{i,t-1}=\Delta \epsilon_{it}\)</span> for <span class="math inline">\(t=2,...,T\)</span>) can be consistently estimated by pooled OLS. This is called the <em>first-difference</em> or FD estimator. Its relative efficiency, and so reasons for choosing it against other consistent alternatives, depends on the properties of the error term. The FD estimator is usually preferred if the errors <span class="math inline">\(u_{it}\)</span> are strongly persistent in time, because then the <span class="math inline">\(\Delta u_{it}\)</span> will tend to be serially uncorrelated.</p>
<p>Lastly, the <em>between</em> model, which is computed on time (group) averages of the data, discards all the information due to intragroup variability but is consistent in some settings (e.g., non-stationarity) where the others are not, and is often preferred to estimate long-run relationships.</p>
<p>Variable coefficients models relax the assumption that <span class="math inline">\(\beta_{it}=\beta\)</span> for all <span class="math inline">\(i,t\)</span>. Fixed coefficients models allow the coefficients to vary along one dimension, like <span class="math inline">\(\beta_{it}=\beta_i\)</span> for all <span class="math inline">\(t\)</span>. Random coefficients models instead assume that coefficients vary randomly around a common average, as <span class="math inline">\(\beta_{it}=\beta+\eta_{i}\)</span> for all <span class="math inline">\(t\)</span>, where <span class="math inline">\(\eta_{i}\)</span> is a group– (time–) specific effect with mean zero.</p>
<!--
The hypotheses on parameters and error terms (and hence the choice of
the most appropriate estimator) are usually tested by means of: *pooling*
tests to check poolability, i.e., the hypothesis that the
same coefficients apply across all individuals; and random effects
tests, comparing the null of spherical residuals with the alternative
of group (time) specific effects in the error term. If the homogeneity
assumption over the $\beta$s is established, the next step is to
establish the presence of unobserved effects. The choice between fixed
and random effects specifications is based on Hausman-type tests,
comparing the two estimators under the null of no significant
difference: if this is not rejected, the more efficient random effects
estimator is chosen.  Even after this step, departures of the error
structure from sphericity can further affect inference, so that either
screening tests or robust diagnostics are needed.
-->
<p>The hypotheses on parameters and error terms (and hence the choice of the most appropriate estimator) are usually tested by means of:</p>
<ul>
<li><em>pooling</em> tests to check poolability, i.e., the hypothesis that the same coefficients apply across all individuals,</li>
<li>if the homogeneity assumption over the coefficients is established, the next step is to establish the presence of unobserved effects, comparing the null of spherical residuals with the alternative of group (time) specific effects in the error term,</li>
<li>the choice between fixed and random effects specifications is based on Hausman-type tests, comparing the two estimators under the null of no significant difference: if this is not rejected, the more efficient random effects estimator is chosen,</li>
<li>even after this step, departures of the error structure from sphericity can further affect inference, so that either screening tests or robust diagnostics are needed.</li>
</ul>
<!--
Dynamic models in a fixed or random effects setting, and in general
lack of strict exogeneity of the regressors, pose further problems to
estimation which are usually dealt with in the generalized method of
moments (GMM) framework.
-->
<p>Dynamic models and in general lack of strict exogeneity of the regressors, pose further problems to estimation which are usually dealt with in the generalized method of moments (GMM) framework.</p>
<p>These were, in our opinion, the basic requirements of a panel data econometrics package for the <code>R</code> language and environment. Some, as often happens with <code>R</code>, were already fulfilled by packages developed for other branches of computational statistics, while others (like the fixed effects or the between estimators) were straightforward to compute after transforming the data, but in every case there were either language inconsistencies w.r.t. the standard econometric toolbox or subtleties to be dealt with (like, for example, appropriate computation of standard errors for the demeaned model, a common pitfall), so we felt there was need for an “all in one” econometrics-oriented package allowing to make specification searches, estimation and inference in a natural way.</p>
</div>
<div id="software-approach" class="section level1">
<h1>Software approach</h1>
<div id="data-structure" class="section level2">
<h2>Data structure</h2>
<p>Panel data have a special structure: each row of the data corresponds to a specific individual and time period. In <code>plm</code> the <code>data</code> argument may be an ordinary <code>data.frame</code> but, in this case, an argument called <code>index</code> has to be added to indicate the structure of the data. This can be:</p>
<ul>
<li><code>NULL</code> (the default value), it is then assumed that the first two columns contain the individual and the time index and that observations are ordered by individual and by time period,</li>
<li>a character string, which should be the name of the individual index,</li>
<li>a character vector of length two containing the names of the individual and the time index,</li>
<li>an integer which is the number of individuals (only in case of a balanced panel with observations ordered by individual).</li>
</ul>
<p>The <code>pdata.frame</code> function is then called internally, which returns a <code>pdata.frame</code> which is a <code>data.frame</code> with an attribute called index. This attribute is a <code>data.frame</code> that contains the individual and the time indexes.</p>
<p>It is also possible to use directly the <code>pdata.frame</code> function and then to use the <code>pdata.frame</code> in the estimation functions.</p>
</div>
<div id="interface" class="section level2">
<h2>Interface</h2>
<div id="estimation-interface" class="section level3">
<h3>Estimation interface</h3>
<p><code>plm</code> provides various functions for estimation, among them:</p>
<ul>
<li><code>plm</code>: estimation of the basic panel models, <em>i.e.</em>, within, between and random effect models. Models are estimated using the <code>lm</code> function to transformed data,</li>
<li><code>pvcm</code>: estimation of models with variable coefficients,</li>
<li><code>pgmm</code>: estimation of generalized method of moments models,</li>
<li><code>pggls</code>: estimation of general feasible generalized least squares models,</li>
<li><code>pmg</code>: estimators for mean groups (MG), demeaned MG (DMG) and common correlated effects MG (CCEMG) for heterogeneous panel models,</li>
<li><code>pcce</code>: estimators for common correlated effects mean groups (CCEMG) and pooled (CCEP) for panel data with common factors,</li>
<li><code>pldv</code>: panel estimators for limited dependent variables.</li>
</ul>
<p>The interface of these functions is consistent with the <code>lm()</code> function. Namely, their first two arguments are <code>formula</code> and <code>data</code> (which should be a <code>data.frame</code> and is mandatory). Three additional arguments are common to these functions:</p>
<ul>
<li><code>index</code>: this argument enables the estimation functions to identify the structure of the data, <em>i.e.</em>, the individual and the time period for each observation,</li>
<li><code>effect</code>: the kind of effects to include in the model, <em>i.e.</em>, individual effects, time effects or both<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>,</li>
<li><code>model</code>: the kind of model to be estimated, most of the time a model with fixed effects or a model with random effects.</li>
</ul>
<p>The results of these four functions are stored in an object which class has the same name of the function. They all inherit from class <code>panelmodel</code>. A <code>panelmodel</code> object contains: <code>coefficients</code>, <code>residuals</code>, <code>fitted.values</code>, <code>vcov</code>, <code>df.residual</code> and <code>call</code> and functions that extract these elements are provided.</p>
<!-- TODO: currently, plm objects do not contain fitted.values -->
</div>
<div id="testing-interface" class="section level3">
<h3>Testing interface</h3>
<p>The diagnostic testing interface provides both <code>formula</code> and <code>panelmodel</code> methods for most functions, with some exceptions. The user may thus choose whether to employ results stored in a previously estimated <code>panelmodel</code> object or to re-estimate it for the sake of testing.</p>
<p>Although the first strategy is the most efficient one, diagnostic testing on panel models mostly employs OLS residuals from pooling model objects, whose estimation is computationally inexpensive. Therefore most examples in the following are based on <code>formula</code> methods, which are perhaps the cleanest for illustrative purposes.</p>
<!--
In some cases where tests are to be done on model objects that be, in
computational terms, more demanding to estimate (and that are also
likely to be already present in the workspace at that stage), an
exception is made to the rule and tests require a `panelmodel`
object argument. This is the case of Hausman tests and of
Wooldridge-type serial correlation tests.
-->
</div>
</div>
<div id="computational-approach-to-estimation" class="section level2">
<h2>Computational approach to estimation</h2>
<p>The feasible GLS methods needed for efficient estimation of unobserved effects models have a simple closed-form solution: once the variance components have been estimated and hence the covariance matrix of errors <span class="math inline">\(\hat{V}\)</span>, model parameters can be estimated as</p>
<p><span class="math display">\[\begin{equation}
  (\#eq:naive)
  \hat{\beta}=(X^\top  \hat{V}^{-1} X)^{-1} (X^\top  \hat{V}^{-1} y)
\end{equation}\]</span></p>
<p>Nevertheless, in practice plain computation of <span class="math inline">\(\hat{\beta}\)</span> has long been an intractable problem even for moderate-sized data sets because of the need to invert the <span class="math inline">\(N\times N\)</span> <span class="math inline">\(\hat{V}\)</span> matrix. With the advances in computer power, this is no more so, and it is possible to program the “naive” estimator @ref(eq:naive) in <code>R</code> with standard matrix algebra operators and have it working seamlessly for the standard “guinea pigs”, e.g., the Grunfeld data. Estimation with a couple of thousands of data points also becomes feasible on a modern machine, although excruciatingly slow and definitely not suitable for everyday econometric practice. Memory limits would also be very near because of the storage needs related to the huge <span class="math inline">\(\hat{V}\)</span> matrix. An established solution exists for the random effects model which reduces the problem to an ordinary least squares computation.</p>
<div id="the-quasidemeaning-framework" class="section level3">
<h3>The (quasi–)demeaning framework</h3>
<p>The estimation methods for the basic models in panel data econometrics, the pooled OLS, random effects and fixed effects (or within) models, can all be described inside the OLS estimation framework. In fact, while pooled OLS simply pools data, the standard way of estimating fixed effects models with, say, group (time) effects entails transforming the data by subtracting the average over time (group) to every variable, which is usually termed <em>time-demeaning</em>. In the random effects case, the various feasible GLS estimators which have been put forth to tackle the issue of serial correlation induced by the group-invariant random effect have been proven to be equivalent (as far as estimation of <span class="math inline">\(\beta\)</span>s is concerned) to OLS on <em>partially demeaned</em> data, where partial demeaning is defined as:</p>
<p><span class="math display">\[\begin{equation}
 (\#eq:ldemmodel)
 y_{it} - \theta \bar{y}_i = ( X_{it} - \theta \bar{X}_{i} ) \beta + ( u_{it} - \theta \bar{u}_i )
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta=1-[\sigma_u^2 / (\sigma_u^2 + T \sigma_e^2)]^{1/2}\)</span>, <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{X}\)</span> denote time means of <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span>, and the disturbance <span class="math inline">\(v_{it} - \theta \bar{v}_i\)</span> is homoskedastic and serially uncorrelated. Thus the feasible RE estimate for <span class="math inline">\(\beta\)</span> may be obtained estimating <span class="math inline">\(\hat{\theta}\)</span> and running an OLS regression on the transformed data with <code>lm()</code>. The other estimators can be computed as special cases: for <span class="math inline">\(\theta=1\)</span> one gets the fixed effects estimator, for <span class="math inline">\(\theta=0\)</span> the pooled OLS one.</p>
<p>Moreover, instrumental variable estimators of all these models may also be obtained using several calls to <code>lm()</code>.</p>
<p>For this reason the three above estimators have been grouped inside the same function.</p>
<p>On the output side, a number of diagnostics and a very general coefficients’ covariance matrix estimator also benefits from this framework, as they can be readily calculated applying the standard OLS formulas to the demeaned data, which are contained inside <code>plm</code> objects. This will be the subject of subsection <a href="#inference">inference in the panel model</a>.</p>
</div>
<div id="the-object-oriented-approach-to-general-gls-computations" class="section level3">
<h3>The object oriented approach to general GLS computations</h3>
<p>The covariance matrix of errors in general GLS models is too generic to fit the quasi-demeaning framework, so this method calls for a full-blown application of GLS as in @ref(eq:naive). On the other hand, this estimator relies heavily on <span class="math inline">\(n\)</span>–asymptotics, making it theoretically most suitable for situations which forbid it computationally: e.g., “short” micropanels with thousands of individuals observed over few time periods.</p>
<p><code>R</code> has general facilities for fast matrix computation based on object orientation: particular types of matrices (symmetric, sparse, dense etc.) are assigned the relevant class and the additional information on structure is used in the computations, sometimes with dramatic effects on performance (see <span class="citation">Bates (2004)</span>) and packages <code>Matrix</code> (see <span class="citation">Bates and Maechler (2016)</span>) and <code>SparseM</code> (see <span class="citation">Koenker and Ng (2016)</span>). Some optimized linear algebra routines are available in the <code>R</code> package <code>bdsmatrix</code> (see <span class="citation">Therneau (2014)</span>) which exploit the particular block-diagonal and symmetric structure of <span class="math inline">\(\hat{V}\)</span> making it possible to implement a fast and reliable full-matrix solution to problems of any practically relevant size.</p>
<p>The <span class="math inline">\(\hat{V}\)</span> matrix is constructed as an object of class <code>bdsmatrix</code>. The peculiar properties of this matrix class are used for efficiently storing the object in memory and then by ad-hoc versions of the <code>solve</code> and <code>crossprod</code> methods, dramatically reducing computing times and memory usage. The resulting matrix is then used “the naive way” as in @ref(eq:naive) to compute <span class="math inline">\(\hat{\beta}\)</span>, resulting in speed comparable to that of the demeaning solution.</p>
</div>
</div>
<div id="inference" class="section level2">
<h2>Inference in the panel model</h2>
<p>General frameworks for restrictions and linear hypotheses testing are available in the <code>R</code> environment<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. These are based on the Wald test, constructed as <span class="math inline">\(\hat{\beta}^\top \hat{V}^{-1} \hat{\beta}\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{V}\)</span> are consistent estimates of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(V(\beta)\)</span>, The Wald test may be used for zero-restriction (i.e., significance) testing and, more generally, for linear hypotheses in the form <span class="math inline">\((R \hat{\beta} - r)^\top [R \hat{V} R^\top ]^{-1} (R \hat{\beta} - r)\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. To be applicable, the test functions require extractor methods for coefficients’ and covariance matrix estimates to be defined for the model object to be tested. Model objects in <code>plm</code> all have <code>coef()</code> and <code>vcov()</code> methods and are therefore compatible with the above functions.</p>
<p>In the same framework, robust inference is accomplished substituting (“plugging in”) a robust estimate of the coefficient covariance matrix into the Wald statistic formula. In the panel context, the estimator of choice is the White system estimator. This called for a flexible method for computing robust coefficient covariance matrices <em>`a la White</em> for <code>plm</code> objects.</p>
<p>A general White system estimator for panel data is:</p>
<p><span class="math display">\[\begin{equation*}
  \hat{V}_R(\beta)=(X^\top X)^{-1} \sum_{i=1}^n{X_i^\top  E_i X_i} (X^\top X)^{-1}
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(E_i\)</span> is a function of the residuals <span class="math inline">\(\hat{e}_{it}, \; t=1, \dots T\)</span> chosen according to the relevant heteroskedasticity and correlation structure. Moreover, it turns out that the White covariance matrix calculated on the demeaned model’s regressors and residuals (both part of <code>plm</code> objects) is a consistent estimator of the relevant model’s parameters’ covariance matrix, thus the method is readily applicable to models estimated by random or fixed effects, first difference or pooled OLS methods. Different pre-weighting schemes taken from package <code>sandwich</code> (see <span class="citation">Zeileis (2004)</span>; <span class="citation">Lumley and Zeileis (2015)</span>) are also implemented to improve small-sample performance. Robust estimators with any combination of covariance structures and weighting schemes can be passed on to the testing functions.</p>
</div>
</div>
<div id="dataformula" class="section level1">
<h1>Managing data and formulae</h1>
<p>The package is now illustrated by application to some well-known examples. It is loaded using</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;plm&quot;</span>)</span></code></pre></div>
<p>The four data sets used are <code>EmplUK</code> which was used by <span class="citation">Arellano and Bond (1991)</span>, the <code>Grunfeld</code> data <span class="citation">(Kleiber and Zeileis 2008)</span> which is used in several econometric books, the <code>Produc</code> data used by <span class="citation">Munnell (1990)</span> and the <code>Wages</code> used by <span class="citation">Cornwell and Rupert (1988)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;EmplUK&quot;</span>, <span class="dt">package=</span><span class="st">&quot;plm&quot;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;Produc&quot;</span>, <span class="dt">package=</span><span class="st">&quot;plm&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;Grunfeld&quot;</span>, <span class="dt">package=</span><span class="st">&quot;plm&quot;</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;Wages&quot;</span>, <span class="dt">package=</span><span class="st">&quot;plm&quot;</span>)</span></code></pre></div>
<div id="data-structure-1" class="section level2">
<h2>Data structure</h2>
<p>As observed above, the current version of <code>plm</code> is capable of working with a regular <code>data.frame</code> without any further transformation, provided that the individual and time indexes are in the first two columns, as in all the example data sets but <code>Wages</code>. If this weren’t the case, an <code>index</code> optional argument would have to be passed on to the estimating and testing functions.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">head</span>(Grunfeld)</span></code></pre></div>
<pre><code>##   firm year   inv  value capital
## 1    1 1935 317.6 3078.5     2.8
## 2    1 1936 391.8 4661.7    52.6
## 3    1 1937 410.6 5387.1   156.9
## 4    1 1938 257.7 2792.2   209.2
## 5    1 1939 330.8 4313.2   203.4
## 6    1 1940 461.2 4643.9   207.2</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>E &lt;-<span class="st"> </span><span class="kw">pdata.frame</span>(EmplUK, <span class="dt">index=</span><span class="kw">c</span>(<span class="st">&quot;firm&quot;</span>,<span class="st">&quot;year&quot;</span>), <span class="dt">drop.index=</span><span class="ot">TRUE</span>, <span class="dt">row.names=</span><span class="ot">TRUE</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="kw">head</span>(E)</span></code></pre></div>
<pre><code>##        sector   emp    wage capital   output
## 1-1977      7 5.041 13.1516  0.5894  95.7072
## 1-1978      7 5.600 12.3018  0.6318  97.3569
## 1-1979      7 5.015 12.8395  0.6771  99.6083
## 1-1980      7 4.715 13.8039  0.6171 100.5501
## 1-1981      7 4.093 14.2897  0.5076  99.5581
## 1-1982      7 3.166 14.8681  0.4229  98.6151</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">attr</span>(E, <span class="st">&quot;index&quot;</span>))</span></code></pre></div>
<pre><code>##   firm year
## 1    1 1977
## 2    1 1978
## 3    1 1979
## 4    1 1980
## 5    1 1981
## 6    1 1982</code></pre>
<p>Two further arguments are logical: <code>drop.index = TRUE</code> drops the indexes from the <code>data.frame</code> and <code>row.names = TRUE</code> computes “fancy” row names by pasting the individual and the time indexes. While extracting a series from a <code>pdata.frame</code>, a <code>pseries</code> is created, which is the original series with the index attribute. This object has specific methods, like <code>summary</code> and <code>as.matrix</code>. The former indicates the total variation of the variable and the shares of this variation due to the individual and the time dimensions. The latter gives the matrix representation of the series, with, by default, individuals as rows and times as columns.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="kw">summary</span>(E<span class="op">$</span>emp)</span></code></pre></div>
<pre><code>## total sum of squares: 261539.4 
##          id        time 
## 0.980765381 0.009108488</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">as.matrix</span>(E<span class="op">$</span>emp))</span></code></pre></div>
<pre><code>##     1976   1977   1978   1979   1980   1981   1982   1983 1984
## 1     NA  5.041  5.600  5.015  4.715  4.093  3.166  2.936   NA
## 2     NA 71.319 70.643 70.918 72.031 73.689 72.419 68.518   NA
## 3     NA 19.156 19.440 19.900 20.240 19.570 18.125 16.850   NA
## 4     NA 26.160 26.740 27.280 27.830 27.169 24.504 22.562   NA
## 5 86.677 87.100 87.000 90.400 89.200 82.700 73.700     NA   NA
## 6  0.748  0.766  0.762  0.729  0.731  0.779  0.782     NA   NA</code></pre>
</div>
<div id="data-transformation" class="section level2">
<h2>Data transformation</h2>
<p>Panel data estimation requires to apply different transformations to raw series. If <span class="math inline">\(x\)</span> is a series of length <span class="math inline">\(nT\)</span> (where <span class="math inline">\(n\)</span> is the number of individuals and <span class="math inline">\(T\)</span> is the number of time periods), the transformed series <span class="math inline">\(\tilde{x}\)</span> is obtained as <span class="math inline">\(\tilde{x}=Mx\)</span> where <span class="math inline">\(M\)</span> is a transformation matrix. Denoting <span class="math inline">\(j\)</span> a vector of one of length <span class="math inline">\(T\)</span> and <span class="math inline">\(I_n\)</span> the identity matrix of dimension <span class="math inline">\(n\)</span>, we get:</p>
<ul>
<li>the between transformation: <span class="math inline">\(P=\frac{1}{T}I_n\otimes jj&#39;\)</span> returns a vector containing the individual means. The <code>Between</code> and <code>between</code> functions perform this operation, the first one returning a vector of length <span class="math inline">\(nT\)</span>, the second one a vector of length <span class="math inline">\(n\)</span>,</li>
<li>the within transformation: <span class="math inline">\(Q=I_{nT}-P\)</span> returns a vector containing the values in deviation from the individual means. The <code>Within</code> function performs this operation.</li>
<li>the first difference transformation <span class="math inline">\(D=I_n \otimes d\)</span> where</li>
</ul>
<p><span class="math inline">\(d=\left( \begin{array}{ccccccc} 1 &amp; -1 &amp; 0 &amp; 0 &amp; ... &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; ... &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; -1 &amp; ... &amp; 0 &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ... &amp; 1 &amp; -1 \\ \end{array} \right)\)</span></p>
<p>is of dimension <span class="math inline">\((T-1,T)\)</span>.</p>
<p>Note that <code>R</code>’s <code>diff()</code> and <code>lag()</code> functions don’t compute correctly these transformations for panel data because they are unable to identify when there is a change in individual in the data. Therefore, specific methods for <code>pseries</code> objects have been written in order to handle correctly panel data. Note that compared to the <code>lag()</code> method for <code>ts</code> objects, the order of lags are indicated by a positive integer. Moreover, 0 is a relevant value and a vector argument may be provided:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">lag</span>(E<span class="op">$</span>emp, <span class="dv">0</span><span class="op">:</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>##            0     1     2
## 1-1977 5.041    NA    NA
## 1-1978 5.600 5.041    NA
## 1-1979 5.015 5.600 5.041
## 1-1980 4.715 5.015 5.600
## 1-1981 4.093 4.715 5.015
## 1-1982 3.166 4.093 4.715</code></pre>
<p>Further functions called <code>Between</code>, <code>between</code> and <code>Within</code> are also provided to compute the between and the within transformation. The <code>between</code> returns unique values, whereas <code>Between</code> duplicates the values and returns a vector which length is the number of observations.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">diff</span>(E<span class="op">$</span>emp), <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##     1-1977     1-1978     1-1979     1-1980     1-1981     1-1982     1-1983 
##         NA  0.5590000 -0.5850000 -0.2999997 -0.6220003 -0.9270000 -0.2299998 
##     2-1977     2-1978     2-1979 
##         NA -0.6760020  0.2750010</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">lag</span>(E<span class="op">$</span>emp, <span class="dv">2</span>), <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## 1-1977 1-1978 1-1979 1-1980 1-1981 1-1982 1-1983 2-1977 2-1978 2-1979 
##     NA     NA  5.041  5.600  5.015  4.715  4.093     NA     NA 71.319</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">Within</span>(E<span class="op">$</span>emp))</span></code></pre></div>
<pre><code>##     1-1977     1-1978     1-1979     1-1980     1-1981     1-1982 
##  0.6744285  1.2334285  0.6484285  0.3484288 -0.2735715 -1.2005715</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">between</span>(E<span class="op">$</span>emp), <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##         1         2         3         4 
##  4.366571 71.362428 19.040143 26.035000</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true"></a><span class="kw">head</span>(<span class="kw">Between</span>(E<span class="op">$</span>emp), <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##         1         1         1         1         1         1         1         2 
##  4.366571  4.366571  4.366571  4.366571  4.366571  4.366571  4.366571 71.362428 
##         2         2 
## 71.362428 71.362428</code></pre>
</div>
<div id="formulas" class="section level2">
<h2>Formulas</h2>
<p>In some circumstances, standard <code>formula</code>s are not very useful to describe a model, notably while using instrumental variable like estimators: to deal with these situations, we use the <code>Formula</code> package.</p>
<p>The <code>Formula</code> package provides a class which enables to construct multi-part formula, each part being separated by a pipe sign (<code>|</code>).</p>
<p>The two formulas below are identical:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true"></a>emp <span class="op">~</span><span class="st"> </span>wage <span class="op">+</span><span class="st"> </span>capital <span class="op">|</span><span class="st"> </span><span class="kw">lag</span>(wage,<span class="dv">1</span>) <span class="op">+</span><span class="st"> </span>capital</span></code></pre></div>
<pre><code>## emp ~ wage + capital | lag(wage, 1) + capital</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true"></a>emp <span class="op">~</span><span class="st"> </span>wage <span class="op">+</span><span class="st"> </span>capital <span class="op">|</span><span class="st"> </span>. <span class="op">-</span>wage <span class="op">+</span><span class="st"> </span><span class="kw">lag</span>(wage,<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## emp ~ wage + capital | . - wage + lag(wage, 1)</code></pre>
<p>In the second case, the <code>.</code> means the previous parts which describes the covariates and this part is “updated”. This is particularly interesting when there are a few external instruments.</p>
</div>
</div>
<div id="modelestimation" class="section level1">
<h1>Model estimation</h1>
<div id="estimation-of-the-basic-models-with-plm" class="section level2">
<h2>Estimation of the basic models with plm</h2>
<p>Several models can be estimated with <code>plm</code> by filling the <code>model</code> argument:</p>
<ul>
<li>the fixed effects model (<code>&quot;within&quot;</code>),</li>
<li>the pooling model (<code>&quot;pooling&quot;</code>),</li>
<li>the first-difference model (<code>&quot;fd&quot;</code>),</li>
<li>the between model (<code>&quot;between&quot;</code>),</li>
<li>the error components model (<code>&quot;random&quot;</code>).</li>
</ul>
<p>The basic use of <code>plm</code> is to indicate the model formula, the data and the model to be estimated. For example, the fixed effects model and the random effects model are estimated using:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true"></a>grun.fe &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data =</span> Grunfeld, <span class="dt">model =</span> <span class="st">&quot;within&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true"></a>grun.re &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data =</span> Grunfeld, <span class="dt">model =</span> <span class="st">&quot;random&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true"></a><span class="kw">summary</span>(grun.re)</span></code></pre></div>
<pre><code>## Oneway (individual) effect Random Effect Model 
##    (Swamy-Arora&#39;s transformation)
## 
## Call:
## plm(formula = inv ~ value + capital, data = Grunfeld, model = &quot;random&quot;)
## 
## Balanced Panel: n = 10, T = 20, N = 200
## 
## Effects:
##                   var std.dev share
## idiosyncratic 2784.46   52.77 0.282
## individual    7089.80   84.20 0.718
## theta: 0.8612
## 
## Residuals:
##      Min.   1st Qu.    Median   3rd Qu.      Max. 
## -177.6063  -19.7350    4.6851   19.5105  252.8743 
## 
## Coefficients:
##               Estimate Std. Error z-value Pr(&gt;|z|)    
## (Intercept) -57.834415  28.898935 -2.0013  0.04536 *  
## value         0.109781   0.010493 10.4627  &lt; 2e-16 ***
## capital       0.308113   0.017180 17.9339  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Total Sum of Squares:    2381400
## Residual Sum of Squares: 548900
## R-Squared:      0.7695
## Adj. R-Squared: 0.76716
## Chisq: 657.674 on 2 DF, p-value: &lt; 2.22e-16</code></pre>
<p>For a <code>random</code> model, the <code>summary</code> method gives information about the variance of the components of the errors. Fixed effects may be extracted easily using <code>fixef</code>. An argument <code>type</code> indicates how fixed effects should be computed: in levels <code>type = &quot;level&quot;</code> (the default), in deviations from the overall mean <code>type = &quot;dmean&quot;</code> or in deviations from the first individual <code>type = &quot;dfirst&quot;</code>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true"></a><span class="kw">fixef</span>(grun.fe, <span class="dt">type =</span> <span class="st">&quot;dmean&quot;</span>)</span></code></pre></div>
<pre><code>##         1         2         3         4         5         6         7         8 
##  -11.5528  160.6498 -176.8279   30.9346  -55.8729   35.5826   -7.8095    1.1983 
##         9        10 
##  -28.4783   52.1761</code></pre>
<p>The <code>fixef</code> function returns an object of class <code>fixef</code>. A summary method is provided, which prints the effects (in deviation from the overall intercept), their standard errors and the test of equality to the overall intercept.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true"></a><span class="kw">summary</span>(<span class="kw">fixef</span>(grun.fe, <span class="dt">type =</span> <span class="st">&quot;dmean&quot;</span>))</span></code></pre></div>
<pre><code>##     Estimate Std. Error t-value  Pr(&gt;|t|)    
## 1   -11.5528    49.7080 -0.2324 0.8164700    
## 2   160.6498    24.9383  6.4419 9.627e-10 ***
## 3  -176.8279    24.4316 -7.2377 1.130e-11 ***
## 4    30.9346    14.0778  2.1974 0.0292129 *  
## 5   -55.8729    14.1654 -3.9443 0.0001129 ***
## 6    35.5826    12.6687  2.8087 0.0054998 ** 
## 7    -7.8095    12.8430 -0.6081 0.5438694    
## 8     1.1983    13.9931  0.0856 0.9318489    
## 9   -28.4783    12.8919 -2.2090 0.0283821 *  
## 10   52.1761    11.8269  4.4116 1.725e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>In case of a two-ways effect model, an additional argument <code>effect</code> is required to extract fixed effects:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true"></a>grun.twfe &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>, <span class="dt">effect=</span><span class="st">&quot;twoways&quot;</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true"></a><span class="kw">fixef</span>(grun.twfe, <span class="dt">effect=</span><span class="st">&quot;time&quot;</span>)</span></code></pre></div>
<pre><code>##    1935    1936    1937    1938    1939    1940    1941    1942    1943    1944 
##  -86.90 -106.10 -127.59 -126.13 -156.37 -131.14 -105.70 -108.04 -129.88 -130.00 
##    1945    1946    1947    1948    1949    1950    1951    1952    1953    1954 
## -142.58 -118.07 -126.29 -130.62 -160.40 -162.80 -149.38 -151.53 -154.62 -180.43</code></pre>
</div>
<div id="more-advanced-use-of-plm" class="section level2">
<h2>More advanced use of plm</h2>
<div id="random-effects-estimators" class="section level3">
<h3>Random effects estimators</h3>
<p>As observed above, the random effect model is obtained as a linear estimation on quasi-demeaned data. The parameter of this transformation is obtained using preliminary estimations.</p>
<p>Four estimators of this parameter are available, depending on the value of the argument <code>random.method</code>:</p>
<ul>
<li><code>&quot;swar&quot;</code>: from <span class="citation">Swamy and Arora (1972)</span>, the default value,</li>
<li><code>&quot;walhus&quot;</code>: from <span class="citation">Wallace and Hussain (1969)</span>,</li>
<li><code>&quot;amemiya&quot;</code>: from <span class="citation">Amemiya (1971)</span>,</li>
<li><code>&quot;nerlove&quot;</code>: from <span class="citation">Nerlove (1971)</span>.</li>
</ul>
<p>For example, to use the <code>amemiya</code> estimator:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true"></a>grun.amem &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld,</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true"></a>                 <span class="dt">model=</span><span class="st">&quot;random&quot;</span>, <span class="dt">random.method=</span><span class="st">&quot;amemiya&quot;</span>)</span></code></pre></div>
<p>The estimation of the variance of the error components are performed using the <code>ercomp</code> function, which has a <code>method</code> and an <code>effect</code> argument, and can be used by itself:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true"></a><span class="kw">ercomp</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">method =</span> <span class="st">&quot;amemiya&quot;</span>, <span class="dt">effect =</span> <span class="st">&quot;twoways&quot;</span>)</span></code></pre></div>
<pre><code>##                   var std.dev share
## idiosyncratic 2644.13   51.42 0.256
## individual    7452.02   86.33 0.721
## time           243.78   15.61 0.024
## theta: 0.868 (id) 0.2787 (time) 0.2776 (total)</code></pre>
</div>
<div id="introducing-time-or-two-ways-effects" class="section level3">
<h3>Introducing time or two-ways effects</h3>
<p>The default behavior of <code>plm</code> is to introduce individual effects. Using the <code>effect</code> argument, one may also introduce:</p>
<ul>
<li>time effects (<code>effect = &quot;time&quot;</code>),</li>
<li>individual and time effects (<code>effect = &quot;twoways&quot;</code>).</li>
</ul>
<p>For example, to estimate a two-ways effect model for the <code>Grunfeld</code> data:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true"></a>grun.tways &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data =</span> Grunfeld, <span class="dt">effect =</span> <span class="st">&quot;twoways&quot;</span>,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true"></a>                  <span class="dt">model =</span> <span class="st">&quot;random&quot;</span>, <span class="dt">random.method =</span> <span class="st">&quot;amemiya&quot;</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true"></a><span class="kw">summary</span>(grun.tways)</span></code></pre></div>
<pre><code>## Twoways effects Random Effect Model 
##    (Amemiya&#39;s transformation)
## 
## Call:
## plm(formula = inv ~ value + capital, data = Grunfeld, effect = &quot;twoways&quot;, 
##     model = &quot;random&quot;, random.method = &quot;amemiya&quot;)
## 
## Balanced Panel: n = 10, T = 20, N = 200
## 
## Effects:
##                   var std.dev share
## idiosyncratic 2644.13   51.42 0.256
## individual    7452.02   86.33 0.721
## time           243.78   15.61 0.024
## theta: 0.868 (id) 0.2787 (time) 0.2776 (total)
## 
## Residuals:
##      Min.   1st Qu.    Median   3rd Qu.      Max. 
## -176.9062  -18.0431    3.2697   17.1719  234.1735 
## 
## Coefficients:
##               Estimate Std. Error z-value Pr(&gt;|z|)    
## (Intercept) -63.767791  29.851537 -2.1362  0.03267 *  
## value         0.111386   0.010909 10.2102  &lt; 2e-16 ***
## capital       0.323321   0.018772 17.2232  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Total Sum of Squares:    2066800
## Residual Sum of Squares: 518200
## R-Squared:      0.74927
## Adj. R-Squared: 0.74673
## Chisq: 588.717 on 2 DF, p-value: &lt; 2.22e-16</code></pre>
<p>In the “effects” section of the printed summary of the result, the variance of the three elements of the error term and the three parameters used in the transformation are printed.</p>
</div>
<div id="unbalanced-panels" class="section level3">
<h3>Unbalanced panels</h3>
<p>Estimations by <code>plm</code> support unbalanced panel models.</p>
<p>The following example is using data used by <span class="citation">Harrison and Rubinfeld (1978)</span> to estimate an hedonic housing prices function. It is reproduced in <span class="citation">Baltagi (2005)</span>, p.174/ <span class="citation">Baltagi (2013)</span>, p.197.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;Hedonic&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;plm&quot;</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true"></a>Hed &lt;-<span class="st"> </span><span class="kw">plm</span>(mv<span class="op">~</span>crim<span class="op">+</span>zn<span class="op">+</span>indus<span class="op">+</span>chas<span class="op">+</span>nox<span class="op">+</span>rm<span class="op">+</span>age<span class="op">+</span>dis<span class="op">+</span>rad<span class="op">+</span>tax<span class="op">+</span>ptratio<span class="op">+</span>blacks<span class="op">+</span>lstat,</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true"></a>           <span class="dt">data =</span> Hedonic, <span class="dt">model =</span> <span class="st">&quot;random&quot;</span>, <span class="dt">index =</span> <span class="st">&quot;townid&quot;</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true"></a><span class="kw">summary</span>(Hed)</span></code></pre></div>
<pre><code>## Oneway (individual) effect Random Effect Model 
##    (Swamy-Arora&#39;s transformation)
## 
## Call:
## plm(formula = mv ~ crim + zn + indus + chas + nox + rm + age + 
##     dis + rad + tax + ptratio + blacks + lstat, data = Hedonic, 
##     model = &quot;random&quot;, index = &quot;townid&quot;)
## 
## Unbalanced Panel: n = 92, T = 1-30, N = 506
## 
## Effects:
##                   var std.dev share
## idiosyncratic 0.01696 0.13025 0.562
## individual    0.01324 0.11505 0.438
## theta:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.2505  0.5483  0.6284  0.6141  0.7147  0.7976 
## 
## Residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.62902 -0.06712 -0.00156 -0.00216  0.06858  0.54973 
## 
## Coefficients:
##                Estimate  Std. Error  z-value  Pr(&gt;|z|)    
## (Intercept)  9.6859e+00  1.9751e-01  49.0398 &lt; 2.2e-16 ***
## crim        -7.4120e-03  1.0478e-03  -7.0738 1.508e-12 ***
## zn           7.8877e-05  6.5001e-04   0.1213 0.9034166    
## indus        1.5563e-03  4.0349e-03   0.3857 0.6997051    
## chasyes     -4.4247e-03  2.9212e-02  -0.1515 0.8796041    
## nox         -5.8425e-03  1.2452e-03  -4.6921 2.704e-06 ***
## rm           9.0552e-03  1.1886e-03   7.6182 2.573e-14 ***
## age         -8.5787e-04  4.6793e-04  -1.8333 0.0667541 .  
## dis         -1.4442e-01  4.4094e-02  -3.2753 0.0010557 ** 
## rad          9.5984e-02  2.6611e-02   3.6069 0.0003098 ***
## tax         -3.7740e-04  1.7693e-04  -2.1331 0.0329190 *  
## ptratio     -2.9476e-02  9.0698e-03  -3.2499 0.0011546 ** 
## blacks       5.6278e-01  1.0197e-01   5.5188 3.413e-08 ***
## lstat       -2.9107e-01  2.3927e-02 -12.1650 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Total Sum of Squares:    987.94
## Residual Sum of Squares: 8.9988
## R-Squared:      0.99091
## Adj. R-Squared: 0.99067
## Chisq: 1199.5 on 13 DF, p-value: &lt; 2.22e-16</code></pre>
<p>Measures for the unbalancedness of a panel data set or the data used in estimated models are provided by function <code>punbalancedness</code>. It gives the measures <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\nu\)</span> from <span class="citation">Ahrens and Pincus (1981)</span> where for both 1 represents balanced data and the more unbalanced the data the lower the value.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true"></a><span class="kw">punbalancedness</span>(Hed)</span></code></pre></div>
<pre><code>##     gamma        nu 
## 0.4715336 0.5188292</code></pre>
</div>
<div id="instrumental-variable-estimators" class="section level3">
<h3>Instrumental variable estimators</h3>
<p>All of the models presented above may be estimated using instrumental variables. The instruments are specified at the end of the formula after a <code>|</code> sign (pipe).</p>
<p>The instrumental variables estimator used is indicated with the <code>inst.method</code> argument:</p>
<ul>
<li><code>&quot;bvk&quot;</code>, from <span class="citation">Balestra and Varadharajan–Krishnakumar (1987)</span>, the default value,</li>
<li><code>&quot;baltagi&quot;</code>, from <span class="citation">Baltagi (1981)</span>,</li>
<li><code>&quot;am&quot;</code>, from <span class="citation">Amemiya and MaCurdy (1986)</span>,</li>
<li><code>&quot;bms&quot;</code>, from <span class="citation">Breusch, Mizon, and Schmidt (1989)</span>.</li>
</ul>
<p>An illustration is in the following example from <span class="citation">Baltagi (2005)</span>, p.120 / <span class="citation">Baltagi (2013)</span>, p.137 (“G2SLS”).</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true"></a><span class="kw">data</span>(<span class="st">&quot;Crime&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;plm&quot;</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true"></a>cr &lt;-<span class="st"> </span><span class="kw">plm</span>(lcrmrte <span class="op">~</span><span class="st"> </span>lprbarr <span class="op">+</span><span class="st"> </span>lpolpc <span class="op">+</span><span class="st"> </span>lprbconv <span class="op">+</span><span class="st"> </span>lprbpris <span class="op">+</span><span class="st"> </span>lavgsen <span class="op">+</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true"></a><span class="st">          </span>ldensity <span class="op">+</span><span class="st"> </span>lwcon <span class="op">+</span><span class="st"> </span>lwtuc <span class="op">+</span><span class="st"> </span>lwtrd <span class="op">+</span><span class="st"> </span>lwfir <span class="op">+</span><span class="st"> </span>lwser <span class="op">+</span><span class="st"> </span>lwmfg <span class="op">+</span><span class="st"> </span>lwfed <span class="op">+</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true"></a><span class="st">          </span>lwsta <span class="op">+</span><span class="st"> </span>lwloc <span class="op">+</span><span class="st"> </span>lpctymle <span class="op">+</span><span class="st"> </span>lpctmin <span class="op">+</span><span class="st"> </span>region <span class="op">+</span><span class="st"> </span>smsa <span class="op">+</span><span class="st"> </span><span class="kw">factor</span>(year)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true"></a>          <span class="op">|</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>lprbarr <span class="op">-</span><span class="st"> </span>lpolpc <span class="op">+</span><span class="st"> </span>ltaxpc <span class="op">+</span><span class="st"> </span>lmix,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true"></a>          <span class="dt">data =</span> Crime, <span class="dt">model =</span> <span class="st">&quot;random&quot;</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true"></a><span class="kw">summary</span>(cr)</span></code></pre></div>
<pre><code>## Oneway (individual) effect Random Effect Model 
##    (Swamy-Arora&#39;s transformation)
## Instrumental variable estimation
##    (Balestra-Varadharajan-Krishnakumar&#39;s transformation)
## 
## Call:
## plm(formula = lcrmrte ~ lprbarr + lpolpc + lprbconv + lprbpris + 
##     lavgsen + ldensity + lwcon + lwtuc + lwtrd + lwfir + lwser + 
##     lwmfg + lwfed + lwsta + lwloc + lpctymle + lpctmin + region + 
##     smsa + factor(year) | . - lprbarr - lpolpc + ltaxpc + lmix, 
##     data = Crime, model = &quot;random&quot;)
## 
## Balanced Panel: n = 90, T = 7, N = 630
## 
## Effects:
##                   var std.dev share
## idiosyncratic 0.02227 0.14924 0.326
## individual    0.04604 0.21456 0.674
## theta: 0.7457
## 
## Residuals:
##       Min.    1st Qu.     Median    3rd Qu.       Max. 
## -0.7485357 -0.0709883  0.0040648  0.0784455  0.4756273 
## 
## Coefficients:
##                  Estimate Std. Error z-value  Pr(&gt;|z|)    
## (Intercept)    -0.4538501  1.7029831 -0.2665  0.789852    
## lprbarr        -0.4141383  0.2210496 -1.8735  0.060998 .  
## lpolpc          0.5049461  0.2277778  2.2168  0.026634 *  
## lprbconv       -0.3432506  0.1324648 -2.5913  0.009563 ** 
## lprbpris       -0.1900467  0.0733392 -2.5913  0.009560 ** 
## lavgsen        -0.0064389  0.0289407 -0.2225  0.823935    
## ldensity        0.4343449  0.0711496  6.1047 1.030e-09 ***
## lwcon          -0.0042958  0.0414226 -0.1037  0.917403    
## lwtuc           0.0444589  0.0215448  2.0636  0.039060 *  
## lwtrd          -0.0085579  0.0419829 -0.2038  0.838476    
## lwfir          -0.0040305  0.0294569 -0.1368  0.891166    
## lwser           0.0105602  0.0215823  0.4893  0.624630    
## lwmfg          -0.2018020  0.0839373 -2.4042  0.016208 *  
## lwfed          -0.2134579  0.2151046 -0.9923  0.321029    
## lwsta          -0.0601232  0.1203149 -0.4997  0.617275    
## lwloc           0.1835363  0.1396775  1.3140  0.188846    
## lpctymle       -0.1458703  0.2268086 -0.6431  0.520131    
## lpctmin         0.1948763  0.0459385  4.2421 2.214e-05 ***
## regionwest     -0.2281821  0.1010260 -2.2586  0.023905 *  
## regioncentral  -0.1987703  0.0607475 -3.2721  0.001068 ** 
## smsayes        -0.2595451  0.1499718 -1.7306  0.083518 .  
## factor(year)82  0.0132147  0.0299924  0.4406  0.659500    
## factor(year)83 -0.0847693  0.0320010 -2.6490  0.008074 ** 
## factor(year)84 -0.1062027  0.0387893 -2.7379  0.006183 ** 
## factor(year)85 -0.0977457  0.0511681 -1.9103  0.056097 .  
## factor(year)86 -0.0719451  0.0605819 -1.1876  0.235004    
## factor(year)87 -0.0396595  0.0758531 -0.5228  0.601081    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Total Sum of Squares:    30.169
## Residual Sum of Squares: 12.419
## R-Squared:      0.5923
## Adj. R-Squared: 0.57472
## Chisq: 542.478 on 26 DF, p-value: &lt; 2.22e-16</code></pre>
<p>The Hausman-Taylor model (see <span class="citation">Hausman and Taylor (1981)</span>) may be estimated with the <code>plm</code> function by setting parameters accordingly like in the example below. The following replicates <span class="citation">Baltagi (2005)</span>, p. 130/ <span class="citation">Baltagi (2013)</span>, p. 146.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true"></a><span class="co">## (note: function pht is a deprecated way to estimate this type of model</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true"></a><span class="co">## ht &lt;- pht(lwage~wks+south+smsa+married+exp+I(exp^2)+</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true"></a><span class="co">##           bluecol+ind+union+sex+black+ed | </span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true"></a><span class="co">##           sex+black+bluecol+south+smsa+ind,</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true"></a><span class="co">##           data=Wages,index=595)</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true"></a>ht &lt;-<span class="st"> </span><span class="kw">plm</span>(lwage <span class="op">~</span><span class="st"> </span>wks <span class="op">+</span><span class="st"> </span>south <span class="op">+</span><span class="st"> </span>smsa <span class="op">+</span><span class="st"> </span>married <span class="op">+</span><span class="st"> </span>exp <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(exp <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true"></a><span class="st">              </span>bluecol <span class="op">+</span><span class="st"> </span>ind <span class="op">+</span><span class="st"> </span>union <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>black <span class="op">+</span><span class="st"> </span>ed <span class="op">|</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true"></a><span class="st">              </span>bluecol <span class="op">+</span><span class="st"> </span>south <span class="op">+</span><span class="st"> </span>smsa <span class="op">+</span><span class="st"> </span>ind <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>black <span class="op">|</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true"></a><span class="st">              </span>wks <span class="op">+</span><span class="st"> </span>married <span class="op">+</span><span class="st"> </span>union <span class="op">+</span><span class="st"> </span>exp <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(exp <span class="op">^</span><span class="st"> </span><span class="dv">2</span>), </span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true"></a>          <span class="dt">data =</span> Wages, <span class="dt">index =</span> <span class="dv">595</span>,</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true"></a>          <span class="dt">random.method =</span> <span class="st">&quot;ht&quot;</span>, <span class="dt">model =</span> <span class="st">&quot;random&quot;</span>, <span class="dt">inst.method =</span> <span class="st">&quot;baltagi&quot;</span>)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true"></a><span class="kw">summary</span>(ht)</span></code></pre></div>
<pre><code>## Oneway (individual) effect Random Effect Model 
##    (Hausman-Taylor&#39;s transformation)
## Instrumental variable estimation
##    (Baltagi&#39;s transformation)
## 
## Call:
## plm(formula = lwage ~ wks + south + smsa + married + exp + I(exp^2) + 
##     bluecol + ind + union + sex + black + ed | bluecol + south + 
##     smsa + ind + sex + black | wks + married + union + exp + 
##     I(exp^2), data = Wages, model = &quot;random&quot;, random.method = &quot;ht&quot;, 
##     inst.method = &quot;baltagi&quot;, index = 595)
## 
## Balanced Panel: n = 595, T = 7, N = 4165
## 
## Effects:
##                   var std.dev share
## idiosyncratic 0.02304 0.15180 0.025
## individual    0.88699 0.94180 0.975
## theta: 0.9392
## 
## Residuals:
##       Min.    1st Qu.     Median    3rd Qu.       Max. 
## -12.643736  -0.466002   0.043285   0.524739  13.340263 
## 
## Coefficients:
##                Estimate  Std. Error z-value  Pr(&gt;|z|)    
## (Intercept)  2.9127e+00  2.8365e-01 10.2687 &lt; 2.2e-16 ***
## wks          8.3740e-04  5.9973e-04  1.3963   0.16263    
## southyes     7.4398e-03  3.1955e-02  0.2328   0.81590    
## smsayes     -4.1833e-02  1.8958e-02 -2.2066   0.02734 *  
## marriedyes  -2.9851e-02  1.8980e-02 -1.5728   0.11578    
## exp          1.1313e-01  2.4710e-03 45.7851 &lt; 2.2e-16 ***
## I(exp^2)    -4.1886e-04  5.4598e-05 -7.6718 1.696e-14 ***
## bluecolyes  -2.0705e-02  1.3781e-02 -1.5024   0.13299    
## ind          1.3604e-02  1.5237e-02  0.8928   0.37196    
## unionyes     3.2771e-02  1.4908e-02  2.1982   0.02794 *  
## sexfemale   -1.3092e-01  1.2666e-01 -1.0337   0.30129    
## blackyes    -2.8575e-01  1.5570e-01 -1.8352   0.06647 .  
## ed           1.3794e-01  2.1248e-02  6.4919 8.474e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Total Sum of Squares:    243.04
## Residual Sum of Squares: 4163.6
## R-Squared:      0.60945
## Adj. R-Squared: 0.60833
## Chisq: 6891.87 on 12 DF, p-value: &lt; 2.22e-16</code></pre>
</div>
</div>
<div id="variable-coefficients-model" class="section level2">
<h2>Variable coefficients model</h2>
<p>The <code>pvcm</code> function enables the estimation of variable coefficients models. Time or individual effects are introduced if <code>effect</code> is fixed to <code>&quot;time&quot;</code> or <code>&quot;individual&quot;</code> (the default value).</p>
<p>Coefficients are assumed to be fixed if <code>model=&quot;within&quot;</code> or random if <code>model=&quot;random&quot;</code>. In the first case, a different model is estimated for each individual (or time period). In the second case, the Swamy model (see <span class="citation">Swamy (1970)</span>) model is estimated. It is a generalized least squares model which uses the results of the previous model. Denoting <span class="math inline">\(\hat{\beta}_i\)</span> the vectors of coefficients obtained for each individual, we get:</p>
<p><span class="math display">\[\begin{equation*}
\hat{\beta}=\left(\sum_{i=1}^n \left(\hat{\Delta}+\hat{\sigma}_i^2(X_i^\top X_i)^{-1}\right)^{-1}\right)\left(\hat{\Delta}+\hat{\sigma}_i^2(X_i^\top X_i)^{-1}\right)^{-1}\hat{\beta}_i
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(\hat{\sigma}_i^2\)</span> is the unbiased estimator of the variance of the errors for individual <span class="math inline">\(i\)</span> obtained from the preliminary estimation and:</p>
<p><span class="math display">\[\begin{equation*}
  \hat{\Delta}=\frac{1}{n-1}\sum_{i=1}^n\left(\hat{\beta}_i-\frac{1}{n}\sum_{i=1}^n\hat{\beta}_i\right)
  \left(\hat{\beta}_i-\frac{1}{n}\sum_{i=1}^n\hat{\beta}_i\right)^\top -\frac{1}{n}\sum_{i=1}^n\hat{\sigma}_i^2(X_i^\top X_i)^{-1}
\end{equation*}\]</span></p>
<p>If this matrix is not positive-definite, the second term is dropped.</p>
<p>With the <code>Grunfeld</code> data, we get:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true"></a>grun.varw &lt;-<span class="st"> </span><span class="kw">pvcm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true"></a>grun.varr &lt;-<span class="st"> </span><span class="kw">pvcm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;random&quot;</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true"></a><span class="kw">summary</span>(grun.varr)</span></code></pre></div>
<pre><code>## Oneway (individual) effect Random coefficients model
## 
## Call:
## pvcm(formula = inv ~ value + capital, data = Grunfeld, model = &quot;random&quot;)
## 
## Balanced Panel: n = 10, T = 20, N = 200
## 
## Residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -211.486  -32.321   -4.283    9.048   12.714  579.216 
## 
## Estimated mean of the coefficients:
##              Estimate Std. Error z-value  Pr(&gt;|z|)    
## (Intercept) -9.629285  17.035040 -0.5653 0.5718946    
## value        0.084587   0.019956  4.2387 2.248e-05 ***
## capital      0.199418   0.052653  3.7874 0.0001522 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Estimated variance of the coefficients:
##             (Intercept)      value    capital
## (Intercept)  2344.24402 -0.6852340 -4.0276612
## value          -0.68523  0.0031182 -0.0011847
## capital        -4.02766 -0.0011847  0.0244824
## 
## Total Sum of Squares: 474010000
## Residual Sum of Squares: 2194300
## Multiple R-Squared: 0.99537
## Chisq: 38.8364 on 2 DF, p-value: 3.6878e-09</code></pre>
</div>
<div id="generalized-method-of-moments-estimator" class="section level2">
<h2>Generalized method of moments estimator</h2>
<p>The generalized method of moments is mainly used in panel data econometrics to estimate dynamic models <span class="citation">(Arellano and Bond 1991; Holtz–Eakin, Newey, and Rosen 1988)</span>.</p>
<p><span class="math display">\[\begin{equation*}
y_{it}=\rho y_{it-1}+\beta^\top x_{it}+\mu_i+\epsilon_{it}
\end{equation*}\]</span></p>
<p>The model is first differenced to get rid of the individual effect:</p>
<p><span class="math display">\[\begin{equation*}
\Delta y_{it}=\rho \Delta  y_{it-1}+\beta^\top \Delta x_{it}+\Delta \epsilon_{it}
\end{equation*}\]</span></p>
<p>Least squares are inconsistent because <span class="math inline">\(\Delta \epsilon_{it}\)</span> is correlated with <span class="math inline">\(\Delta y_{it-1}\)</span>. <span class="math inline">\(y_{it-2}\)</span> is a valid, but weak instrument (see <span class="citation">Anderson and Hsiao (1981)</span>). The GMM estimator uses the fact that the number of valid instruments is growing with <span class="math inline">\(t\)</span>:</p>
<ul>
<li><span class="math inline">\(t=3\)</span>: <span class="math inline">\(y_1\)</span>,</li>
<li><span class="math inline">\(t=4\)</span>: <span class="math inline">\(y_1,y_2\)</span>,</li>
<li><span class="math inline">\(t=5\)</span>: <span class="math inline">\(y_1,y_2,y_3\)</span>.</li>
</ul>
<p>For individual <span class="math inline">\(i\)</span>, the matrix of instruments is then:</p>
<p><span class="math display">\[\begin{equation*}
W_i=\left(
\begin{array}{ccccccccccccc}
y_1 &amp; 0   &amp; 0   &amp; 0    &amp; 0   &amp; 0   &amp; ... &amp; 0    &amp; 0   &amp;  0   &amp;        0      &amp; x_{i3} \\
0   &amp; y_1 &amp; y_2 &amp; 0    &amp; 0   &amp; 0   &amp; ... &amp; 0    &amp; 0     &amp;  0  &amp;      0          &amp; x_{i4} \\
0   &amp; 0   &amp; 0   &amp; y_1  &amp; y_2 &amp; y_3 &amp; ... &amp; 0    &amp;  0    &amp; 0  &amp; 0                     &amp; x_{i5} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots  &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0   &amp; 0   &amp; 0   &amp; 0    &amp; ...   &amp; ...    &amp;  ...      &amp; y_1  &amp; y_2 &amp; ... &amp; y_{t-2} &amp; x_{iT-2} &amp;\\
\end{array}
\right)
\end{equation*}\]</span></p>
<p>The moment conditions are: <span class="math inline">\(\sum_{i=1}^n W_i^\top e_i(\beta)\)</span> where <span class="math inline">\(e_i(\beta)\)</span> is the vector of residuals for individual <span class="math inline">\(i\)</span>. The GMM estimator minimizes:</p>
<p><span class="math display">\[\begin{equation*}
\left(\sum_{i=1}^n e_i(\beta)^\top W_i\right) A \left(\sum_{i=1}^n W_i^\top e_i(\beta)\right)
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(A\)</span> is the weighting matrix of the moments.</p>
<p>One-step estimators are computed using a known weighting matrix. For the model in first differences, one uses:</p>
<p><span class="math display">\[\begin{equation*}
A^{(1)}=\left(\sum_{i=1}^n W_i^\top H^{(1)}W_i\right)^{-1}
\end{equation*}\]</span></p>
<p>with:</p>
<p><span class="math display">\[\begin{equation*}
H^{(1)}=d^\top d=\left(
\begin{array}{ccccc}
2 &amp; -1 &amp; 0 &amp; ... &amp; 0\\
-1 &amp; 2 &amp; -1 &amp; ... &amp; 0\\
0 &amp; -1 &amp; 2 &amp; ... &amp; 0\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; -1  &amp; 2\\
\end{array}
\right)
\end{equation*}\]</span></p>
<p>Two-steps estimators are obtained using <span class="math inline">\(H^{(2)}_i=\sum_{i=1}^n e^{(1)}_i e^{(1)\top }_i\)</span> where <span class="math inline">\(e_i^{(1)}\)</span> are the residuals of the one step estimate.</p>
<p><span class="citation">Blundell and Bond (1998)</span> show that with weak hypothesis on the data generating process, supplementary moment conditions exist for the equation in level:</p>
<p><span class="math display">\[
y_{it} = \gamma y_{it-1}+\mu_i+\eta_{it}
\]</span></p>
<p>More precisely, they show that <span class="math inline">\(\Delta y_{it-2}=y_{it-2}-y_{it-3}\)</span> is a valid instrument. The estimator is obtained using the residual vector in difference and in level:</p>
<p><span class="math display">\[
e^+_i=(\Delta e_i, e_i)
\]</span></p>
<p>and the matrix of augmented moments:</p>
<p><span class="math display">\[
Z_i^+=\left(
\begin{array}{ccccc}
Z_i &amp; 0   &amp; 0      &amp;  ... &amp; 0  \\
0 &amp; \Delta y_{i2} &amp; 0  &amp; ... &amp; 0 \\
0 &amp; 0 &amp; \Delta y_{i3}   &amp; ... &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; ... &amp; \Delta y_{iT-1}
\end{array}
\right)
\]</span></p>
<p>The moment conditions are then</p>
<p><span class="math display">\[\begin{eqnarray*}
\left(\sum_{i=1}^n Z_i^{+\top} \left(\begin{array}{c}\bar{e}_i(\beta)\\ e_i(\beta)\end{array}\right)\right)^\top = \left(\sum_{i=1}^n y_{i1} \bar{e}_{i3},\sum_{i=1}^n  y_{i1}\bar{e}_{i4},\sum_{i=1}^n  y_{i2}\bar{e}_{i4}, ...,  \right.\\ \left. \sum_{i=1}^n y_{i1} \bar{e}_{iT}, \sum_{i=1}^n y_{i2} \bar{e}_{iT}, ...,\sum_{i=1}^n y_{iT-2} \bar{e}_{iT}, \sum_{i=1}^n \sum_{t=3}^T x_{it} \bar{e}_{it}\right.\\
\left.\sum_{i=1}^n e_{i3} \Delta y_{i2}, \sum_{i=1}^n e_{i4} \Delta y_{i3},  ... , \sum_{i=1}^n e_{iT} \Delta y_{iT-1} \right)^\top
\end{eqnarray*}\]</span></p>
<p>The GMM estimator is provided by the <code>pgmm</code> function. By using a multi-part formula, the variables of the model and the lag structure are described.</p>
<p>In a GMM estimation, there are “normal instruments” and “GMM instruments”. GMM instruments are indicated in the second part of the formula. By default, all the variables of the model that are not used as GMM instruments are used as normal instruments, with the same lag structure; “normal” instruments may also be indicated in the third part of the formula.</p>
<p>The <code>effect</code> argument is either <code>NULL</code>, <code>&quot;individual&quot;</code> (the default), or <code>&quot;twoways&quot;</code>. In the first case, the model is estimated in levels. In the second case, the model is estimated in first differences to get rid of the individuals effects. In the last case, the model is estimated in first differences and time dummies are included.</p>
<p>The <code>model</code> argument specifies whether a one-step or a two-steps model is requested (<code>&quot;onestep&quot;</code> or <code>&quot;twosteps&quot;</code>).</p>
<p>The following example is from <span class="citation">Arellano and Bond (1991)</span>. Employment is explained by past values of employment (two lags), current and first lag of wages and output and current value of capital.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true"></a>emp.gmm &lt;-<span class="st"> </span><span class="kw">pgmm</span>(<span class="kw">log</span>(emp)<span class="op">~</span><span class="kw">lag</span>(<span class="kw">log</span>(emp), <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">lag</span>(<span class="kw">log</span>(wage), <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>)<span class="op">+</span><span class="kw">log</span>(capital)<span class="op">+</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true"></a><span class="st">                </span><span class="kw">lag</span>(<span class="kw">log</span>(output), <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>)<span class="op">|</span><span class="kw">lag</span>(<span class="kw">log</span>(emp), <span class="dv">2</span><span class="op">:</span><span class="dv">99</span>),</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true"></a>                <span class="dt">data =</span> EmplUK, <span class="dt">effect =</span> <span class="st">&quot;twoways&quot;</span>, <span class="dt">model =</span> <span class="st">&quot;twosteps&quot;</span>)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true"></a><span class="kw">summary</span>(emp.gmm)</span></code></pre></div>
<pre><code>## Twoways effects Two steps model
## 
## Call:
## pgmm(formula = log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 
##     0:1) + log(capital) + lag(log(output), 0:1) | lag(log(emp), 
##     2:99), data = EmplUK, effect = &quot;twoways&quot;, model = &quot;twosteps&quot;)
## 
## Unbalanced Panel: n = 140, T = 7-9, N = 1031
## 
## Number of Observations Used: 611
## 
## Residuals:
##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -0.6190677 -0.0255683  0.0000000 -0.0001339  0.0332013  0.6410272 
## 
## Coefficients:
##                         Estimate Std. Error z-value  Pr(&gt;|z|)    
## lag(log(emp), 1:2)1     0.474151   0.185398  2.5575 0.0105437 *  
## lag(log(emp), 1:2)2    -0.052967   0.051749 -1.0235 0.3060506    
## lag(log(wage), 0:1)0   -0.513205   0.145565 -3.5256 0.0004225 ***
## lag(log(wage), 0:1)1    0.224640   0.141950  1.5825 0.1135279    
## log(capital)            0.292723   0.062627  4.6741 2.953e-06 ***
## lag(log(output), 0:1)0  0.609775   0.156263  3.9022 9.530e-05 ***
## lag(log(output), 0:1)1 -0.446373   0.217302 -2.0542 0.0399605 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Sargan test: chisq(25) = 30.11247 (p-value = 0.22011)
## Autocorrelation test (1): normal = -1.53845 (p-value = 0.12394)
## Autocorrelation test (2): normal = -0.2796829 (p-value = 0.77972)
## Wald test for coefficients: chisq(7) = 142.0353 (p-value = &lt; 2.22e-16)
## Wald test for time dummies: chisq(6) = 16.97046 (p-value = 0.0093924)</code></pre>
<p>The following example is from <span class="citation">Blundell and Bond (1998)</span>. The “sys” estimator is obtained using <code>transformation = &quot;ld&quot;</code> for level and difference. The <code>robust</code> argument of the <code>summary</code> method enables to use the robust covariance matrix proposed by <span class="citation">Windmeijer (2005)</span>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true"></a>z2 &lt;-<span class="st"> </span><span class="kw">pgmm</span>(<span class="kw">log</span>(emp) <span class="op">~</span><span class="st"> </span><span class="kw">lag</span>(<span class="kw">log</span>(emp), <span class="dv">1</span>)<span class="op">+</span><span class="st"> </span><span class="kw">lag</span>(<span class="kw">log</span>(wage), <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true"></a><span class="st">           </span><span class="kw">lag</span>(<span class="kw">log</span>(capital), <span class="dv">0</span><span class="op">:</span><span class="dv">1</span>) <span class="op">|</span><span class="st"> </span><span class="kw">lag</span>(<span class="kw">log</span>(emp), <span class="dv">2</span><span class="op">:</span><span class="dv">99</span>) <span class="op">+</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true"></a><span class="st">           </span><span class="kw">lag</span>(<span class="kw">log</span>(wage), <span class="dv">2</span><span class="op">:</span><span class="dv">99</span>) <span class="op">+</span><span class="st"> </span><span class="kw">lag</span>(<span class="kw">log</span>(capital), <span class="dv">2</span><span class="op">:</span><span class="dv">99</span>),        </span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true"></a>           <span class="dt">data =</span> EmplUK, <span class="dt">effect =</span> <span class="st">&quot;twoways&quot;</span>, <span class="dt">model =</span> <span class="st">&quot;onestep&quot;</span>, </span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true"></a>           <span class="dt">transformation =</span> <span class="st">&quot;ld&quot;</span>)</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true"></a><span class="kw">summary</span>(z2, <span class="dt">robust =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>## Twoways effects One step model
## 
## Call:
## pgmm(formula = log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + 
##     lag(log(capital), 0:1) | lag(log(emp), 2:99) + lag(log(wage), 
##     2:99) + lag(log(capital), 2:99), data = EmplUK, effect = &quot;twoways&quot;, 
##     model = &quot;onestep&quot;, transformation = &quot;ld&quot;)
## 
## Unbalanced Panel: n = 140, T = 7-9, N = 1031
## 
## Number of Observations Used: 1642
## 
## Residuals:
##       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
## -0.7530341 -0.0369030  0.0000000  0.0002882  0.0466069  0.6001503 
## 
## Coefficients:
##                          Estimate Std. Error z-value  Pr(&gt;|z|)    
## lag(log(emp), 1)         0.935605   0.026295 35.5810 &lt; 2.2e-16 ***
## lag(log(wage), 0:1)0    -0.630976   0.118054 -5.3448 9.050e-08 ***
## lag(log(wage), 0:1)1     0.482620   0.136887  3.5257 0.0004224 ***
## lag(log(capital), 0:1)0  0.483930   0.053867  8.9838 &lt; 2.2e-16 ***
## lag(log(capital), 0:1)1 -0.424393   0.058479 -7.2572 3.952e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Sargan test: chisq(100) = 118.763 (p-value = 0.097096)
## Autocorrelation test (1): normal = -4.808434 (p-value = 1.5212e-06)
## Autocorrelation test (2): normal = -0.2800133 (p-value = 0.77947)
## Wald test for coefficients: chisq(5) = 11174.82 (p-value = &lt; 2.22e-16)
## Wald test for time dummies: chisq(7) = 14.71138 (p-value = 0.039882)</code></pre>
</div>
<div id="general-fgls-models" class="section level2">
<h2>General FGLS models</h2>
<p>General FGLS estimators are based on a two-step estimation process: first an OLS model is estimated, then its residuals <span class="math inline">\(\hat{u}_{it}\)</span> are used to estimate an error covariance matrix more general than the random effects one for use in a feasible-GLS analysis. Formally, the estimated error covariance matrix is <span class="math inline">\(\hat{V}=I_n \otimes \hat{\Omega}\)</span>, with <span class="math display">\[\hat{\Omega}=\sum_{i=1}^n \frac{\hat{u}_{it}
  \hat{u}_{it}^\top }{n} \]</span> (see <span class="citation">Wooldridge (2002)</span> 10.4.3 and 10.5.5).</p>
<p>This framework allows the error covariance structure inside every group (if <code>effect = &quot;individual&quot;</code>) of observations to be fully unrestricted and is therefore robust against any type of intragroup heteroskedasticity and serial correlation. This structure, by converse, is assumed identical across groups and thus general FGLS is inefficient under groupwise heteroskedasticity. Cross-sectional correlation is excluded a priori.</p>
<p>Moreover, the number of variance parameters to be estimated with <span class="math inline">\(N=n\times T\)</span> data points is <span class="math inline">\(T(T+1)/2\)</span>, which makes these estimators particularly suited for situations where <span class="math inline">\(n&gt;&gt;T\)</span>, as e.g., in labour or household income surveys, while problematic for “long” panels, where <span class="math inline">\(\hat{V}\)</span> tends to become singular and standard errors therefore become biased downwards.</p>
<p>In a pooled time series context (<code>effect = &quot;time&quot;</code>), symmetrically, this estimator is able to account for arbitrary cross-sectional correlation, provided that the latter is time-invariant (see <span class="citation">Greene (2003)</span> 13.9.1–2, pp. 321–2). In this case serial correlation has to be assumed away and the estimator is consistent with respect to the time dimension, keeping <span class="math inline">\(n\)</span> fixed.</p>
<p>The function <code>pggls</code> estimates general FGLS models, with either fixed or “random” effects<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>.</p>
<p>The “random effect” general FGLS is estimated by:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true"></a>zz &lt;-<span class="st"> </span><span class="kw">pggls</span>(<span class="kw">log</span>(emp)<span class="op">~</span><span class="kw">log</span>(wage)<span class="op">+</span><span class="kw">log</span>(capital), <span class="dt">data=</span>EmplUK, <span class="dt">model=</span><span class="st">&quot;pooling&quot;</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true"></a><span class="kw">summary</span>(zz)</span></code></pre></div>
<pre><code>## Oneway (individual) effect General FGLS model
## 
## Call:
## pggls(formula = log(emp) ~ log(wage) + log(capital), data = EmplUK, 
##     model = &quot;pooling&quot;)
## 
## Unbalanced Panel: n = 140, T = 7-9, N = 1031
## 
## Residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -1.80696 -0.36552  0.06181  0.03230  0.44279  1.58719 
## 
## Coefficients:
##               Estimate Std. Error z-value  Pr(&gt;|z|)    
## (Intercept)   2.023480   0.158468 12.7690 &lt; 2.2e-16 ***
## log(wage)    -0.232329   0.048001 -4.8401 1.298e-06 ***
## log(capital)  0.610484   0.017434 35.0174 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## Total Sum of Squares: 1853.6
## Residual Sum of Squares: 402.55
## Multiple R-squared: 0.78283</code></pre>
<p>The fixed effects <code>pggls</code> (see <span class="citation">Wooldridge (2002)</span>, p. 276) is based on the estimation of a within model in the first step; the rest follows as above. It is estimated by:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true"></a>zz &lt;-<span class="st"> </span><span class="kw">pggls</span>(<span class="kw">log</span>(emp)<span class="op">~</span><span class="kw">log</span>(wage)<span class="op">+</span><span class="kw">log</span>(capital), <span class="dt">data=</span>EmplUK, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span></code></pre></div>
<p>The <code>pggls</code> function is similar to <code>plm</code> in many respects. An exception is that the estimate of the group covariance matrix of errors (<code>zz$sigma</code>, a matrix, not shown) is reported in the model objects instead of the usual estimated variances of the two error components.</p>
</div>
</div>
<div id="tests" class="section level1">
<h1>Tests</h1>
<p>As sketched in Section <a href="#linear-panel-model">linear panel model</a>), specification testing in panel models involves essentially testing for poolability, for individual or time unobserved effects and for correlation between these latter and the regressors (Hausman-type tests). As for the other usual diagnostic checks, we provide a suite of serial correlation tests, while not touching on the issue of heteroskedasticity testing. Instead, we provide heteroskedasticity-robust covariance estimators, to be described in subsection <a href="#robust">robust covariance matrix estimation</a>.</p>
<div id="tests-of-poolability" class="section level2">
<h2>Tests of poolability</h2>
<p><code>pooltest</code> tests the hypothesis that the same coefficients apply to each individual. It is a standard F test, based on the comparison of a model obtained for the full sample and a model based on the estimation of an equation for each individual. The first argument of <code>pooltest</code> is a <code>plm</code> object. The second argument is a <code>pvcm</code> object obtained with <code>model=&quot;within&quot;</code>. If the first argument is a pooling model, the test applies to all the coefficients (including the intercepts), if it is a within model, different intercepts are assumed.</p>
<p>To test the hypothesis that all the coefficients in the <code>Grunfeld</code> example, excluding the intercepts, are equal, we use :</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true"></a>znp &lt;-<span class="st"> </span><span class="kw">pvcm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true"></a>zplm &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true"></a><span class="kw">pooltest</span>(zplm, znp)</span></code></pre></div>
<pre><code>## 
##  F statistic
## 
## data:  inv ~ value + capital
## F = 5.7805, df1 = 18, df2 = 170, p-value = 1.219e-10
## alternative hypothesis: unstability</code></pre>
<p>The same test can be computed using a formula as first argument of the <code>pooltest</code> function:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true"></a><span class="kw">pooltest</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span></code></pre></div>
</div>
<div id="tests-for-individual-and-time-effects" class="section level2">
<h2>Tests for individual and time effects</h2>
<p><code>plmtest</code> implements Lagrange multiplier tests of individual or/and time effects based on the results of the pooling model. Its main argument is a <code>plm</code> object (the result of a pooling model) or a formula.</p>
<p>Two additional arguments can be added to indicate the kind of test to be computed. The argument <code>type</code> is one of:</p>
<ul>
<li><code>&quot;honda&quot;</code>: <span class="citation">Honda (1985)</span>, the default value,</li>
<li><code>&quot;bp&quot;</code>: <span class="citation">Breusch and Pagan (1980)</span>,</li>
<li><code>&quot;kw&quot;</code>: <span class="citation">King and Wu (1997)</span><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>,</li>
<li><code>&quot;ghm&quot;</code>: <span class="citation">Gourieroux, Holly, and Monfort (1982)</span>.</li>
</ul>
<p>The effects tested are indicated with the <code>effect</code> argument (one of <code>&quot;individual&quot;</code>, <code>&quot;time&quot;</code>, or <code>&quot;twoways&quot;</code>). The test statistics implemented are also suitable for unbalanced panels.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>To test the presence of individual and time effects in the <code>Grunfeld</code> example, using the <span class="citation">Gourieroux, Holly, and Monfort (1982)</span> test, we use:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true"></a>g &lt;-<span class="st"> </span><span class="kw">plm</span>(inv <span class="op">~</span><span class="st"> </span>value <span class="op">+</span><span class="st"> </span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;pooling&quot;</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true"></a><span class="kw">plmtest</span>(g, <span class="dt">effect=</span><span class="st">&quot;twoways&quot;</span>, <span class="dt">type=</span><span class="st">&quot;ghm&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Lagrange Multiplier Test - two-ways effects (Gourieroux, Holly and
##  Monfort) for balanced panels
## 
## data:  inv ~ value + capital
## chibarsq = 798.16, df0 = 0.00, df1 = 1.00, df2 = 2.00, w0 = 0.25, w1 =
## 0.50, w2 = 0.25, p-value &lt; 2.2e-16
## alternative hypothesis: significant effects</code></pre>
<p>or</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true"></a><span class="kw">plmtest</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">effect=</span><span class="st">&quot;twoways&quot;</span>, <span class="dt">type=</span><span class="st">&quot;ghm&quot;</span>)</span></code></pre></div>
<p><code>pFtest</code> computes F tests of effects based on the comparison of the within and the pooling model. Its main arguments are either two <code>plm</code> objects (a pooling and a within model) or a formula.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true"></a>gw &lt;-<span class="st"> </span><span class="kw">plm</span>(inv <span class="op">~</span><span class="st"> </span>value <span class="op">+</span><span class="st"> </span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">effect=</span><span class="st">&quot;twoways&quot;</span>, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true"></a>gp &lt;-<span class="st"> </span><span class="kw">plm</span>(inv <span class="op">~</span><span class="st"> </span>value <span class="op">+</span><span class="st"> </span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;pooling&quot;</span>)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true"></a><span class="kw">pFtest</span>(gw, gp)</span></code></pre></div>
<pre><code>## 
##  F test for twoways effects
## 
## data:  inv ~ value + capital
## F = 17.403, df1 = 28, df2 = 169, p-value &lt; 2.2e-16
## alternative hypothesis: significant effects</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true"></a><span class="kw">pFtest</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">effect=</span><span class="st">&quot;twoways&quot;</span>)</span></code></pre></div>
</div>
<div id="hausman-test" class="section level2">
<h2>Hausman test</h2>
<p><code>phtest</code> computes the Hausman test which is based on the comparison of two sets of estimates (see <span class="citation">Hausman (1978)</span>). Its main arguments are two <code>panelmodel</code> objects or a formula. A classical application of the Hausman test for panel data is to compare the fixed and the random effects models:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true"></a>gw &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true"></a>gr &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;random&quot;</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true"></a><span class="kw">phtest</span>(gw, gr)</span></code></pre></div>
<pre><code>## 
##  Hausman Test
## 
## data:  inv ~ value + capital
## chisq = 2.3304, df = 2, p-value = 0.3119
## alternative hypothesis: one model is inconsistent</code></pre>
</div>
<div id="serialcor" class="section level2">
<h2>Tests of serial correlation</h2>
<p>A model with individual effects has composite errors that are serially correlated by definition. The presence of the time-invariant error component<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> gives rise to serial correlation which does not die out over time, thus standard tests applied on pooled data always end up rejecting the null of spherical residuals<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. There may also be serial correlation of the “usual” kind in the idiosyncratic error terms, e.g., as an AR(1) process. By “testing for serial correlation” we mean testing for this latter kind of dependence.</p>
<p>For these reasons, the subjects of testing for individual error components and for serially correlated idiosyncratic errors are closely related. In particular, simple (<em>marginal</em>) tests for one direction of departure from the hypothesis of spherical errors usually have power against the other one: in case it is present, they are substantially biased towards rejection. <em>Joint</em> tests are correctly sized and have power against both directions, but usually do not give any information about which one actually caused rejection. <em>Conditional</em> tests for serial correlation that take into account the error components are correctly sized under presence of both departures from sphericity and have power only against the alternative of interest. While most powerful if correctly specified, the latter, based on the likelihood framework, are crucially dependent on normality and homoskedasticity of the errors.</p>
<p>In <code>plm</code> we provide a number of joint, marginal and conditional ML-based tests, plus some semiparametric alternatives which are robust vs. heteroskedasticity and free from distributional assumptions.</p>
<div id="unobserved-effects-test" class="section level3">
<h3>Unobserved effects test</h3>
<p>The unobserved effects test <em>`a la Wooldridge</em> (see <span class="citation">Wooldridge (2002)</span> 10.4.4), is a semiparametric test for the null hypothesis that <span class="math inline">\(\sigma^2_{\mu}=0\)</span>, i.e. that there are no unobserved effects in the residuals. Given that under the null the covariance matrix of the residuals for each individual is diagonal, the test statistic is based on the average of elements in the upper (or lower) triangle of its estimate, diagonal excluded: <span class="math inline">\(n^{-1/2} \sum_{i=1}^n \sum_{t=1}^{T-1} \sum_{s=t+1}^T \hat{u}_{it} \hat{u}_{is}\)</span> (where <span class="math inline">\(\hat{u}\)</span> are the pooled OLS residuals), which must be “statistically close” to zero under the null, scaled by its standard deviation: <span class="math display">\[W=\frac{ \sum_{i=1}^n \sum_{t=1}^{T-1} \sum_{s=t+1}^T
  \hat{u}_{it} \hat{u}_{is} }{ [{ \sum_{i=1}^n ( \sum_{t=1}^{T-1}
    \sum_{s=t+1}^T \hat{u}_{it} \hat{u}_{is} } )^2 ]^{1/2} }\]</span></p>
<p>This test is (<span class="math inline">\(n\)</span>-) asymptotically distributed as a standard normal regardless of the distribution of the errors. It does also not rely on homoskedasticity.</p>
<p>It has power both against the standard random effects specification, where the unobserved effects are constant within every group, as well as against any kind of serial correlation. As such, it “nests” both random effects and serial correlation tests, trading some power against more specific alternatives in exchange for robustness.</p>
<p>While not rejecting the null favours the use of pooled OLS, rejection may follow from serial correlation of different kinds, and in particular, quoting <span class="citation">Wooldridge (2002)</span>, “should not be interpreted as implying that the random effects error structure <em>must</em> be true”.</p>
<p>Below, the test is applied to the data and model in <span class="citation">Munnell (1990)</span>:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true"></a><span class="kw">pwtest</span>(<span class="kw">log</span>(gsp)<span class="op">~</span><span class="kw">log</span>(pcap)<span class="op">+</span><span class="kw">log</span>(pc)<span class="op">+</span><span class="kw">log</span>(emp)<span class="op">+</span>unemp, <span class="dt">data=</span>Produc)</span></code></pre></div>
<pre><code>## 
##  Wooldridge&#39;s test for unobserved individual effects
## 
## data:  formula
## z = 3.9383, p-value = 8.207e-05
## alternative hypothesis: unobserved effect</code></pre>
</div>
<div id="locally-robust-tests-for-serial-correlation-or-random-effects" class="section level3">
<h3>Locally robust tests for serial correlation or random effects</h3>
<p>The presence of random effects may affect tests for residual serial correlation, and the opposite. One solution is to use a joint test, which has power against both alternatives. A joint LM test for random effects <em>and</em> serial correlation under normality and homoskedasticity of the idiosyncratic errors has been derived by <span class="citation">Baltagi and Li (1991)</span> and <span class="citation">Baltagi and Li (1995)</span> and is implemented as an option in <code>pbsytest</code>:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true"></a><span class="kw">pbsytest</span>(<span class="kw">log</span>(gsp)<span class="op">~</span><span class="kw">log</span>(pcap)<span class="op">+</span><span class="kw">log</span>(pc)<span class="op">+</span><span class="kw">log</span>(emp)<span class="op">+</span>unemp, <span class="dt">data=</span>Produc, <span class="dt">test=</span><span class="st">&quot;j&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Baltagi and Li AR-RE joint test - balanced panel
## 
## data:  formula
## chisq = 4187.6, df = 2, p-value &lt; 2.2e-16
## alternative hypothesis: AR(1) errors or random effects</code></pre>
<p>Rejection of the joint test, though, gives no information on the direction of the departure from the null hypothesis, i.e.: is rejection due to the presence of serial correlation, of random effects or of both?</p>
<p><span class="citation">Bera, Sosa–Escudero, and Yoon (2001)</span> derive locally robust tests both for individual random effects and for first-order serial correlation in residuals as “corrected” versions of the standard LM test (see <code>plmtest</code>). While still dependent on normality and homoskedasticity, these are robust to <em>local</em> departures from the hypotheses of, respectively, no serial correlation or no random effects. The authors observe that, although suboptimal, these tests may help detecting the right direction of the departure from the null, thus complementing the use of joint tests. Moreover, being based on pooled OLS residuals, the BSY tests are computationally far less demanding than likelihood-based conditional tests.</p>
<p>On the other hand, the statistical properties of these “locally corrected” tests are inferior to those of the non-corrected counterparts when the latter are correctly specified. If there is no serial correlation, then the optimal test for random effects is the likelihood-based LM test of Breusch and Godfrey (with refinements by Honda, see <code>plmtest</code>), while if there are no random effects the optimal test for serial correlation is, again, Breusch-Godfrey’s test<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>. If the presence of a random effect is taken for granted, then the optimal test for serial correlation is the likelihood-based conditional LM test of <span class="citation">Baltagi and Li (1995)</span> (see <code>pbltest</code>).</p>
<p>The serial correlation version is the default:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true"></a><span class="kw">pbsytest</span>(<span class="kw">log</span>(gsp)<span class="op">~</span><span class="kw">log</span>(pcap)<span class="op">+</span><span class="kw">log</span>(pc)<span class="op">+</span><span class="kw">log</span>(emp)<span class="op">+</span>unemp, <span class="dt">data=</span>Produc)</span></code></pre></div>
<pre><code>## 
##  Bera, Sosa-Escudero and Yoon locally robust test - balanced panel
## 
## data:  formula
## chisq = 52.636, df = 1, p-value = 4.015e-13
## alternative hypothesis: AR(1) errors sub random effects</code></pre>
<p>The BSY test for random effects is implemented in the one-sided version<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a>, which takes heed that the variance of the random effect must be non-negative:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true"></a><span class="kw">pbsytest</span>(<span class="kw">log</span>(gsp)<span class="op">~</span><span class="kw">log</span>(pcap)<span class="op">+</span><span class="kw">log</span>(pc)<span class="op">+</span><span class="kw">log</span>(emp)<span class="op">+</span>unemp, <span class="dt">data=</span>Produc, <span class="dt">test=</span><span class="st">&quot;re&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Bera, Sosa-Escudero and Yoon locally robust test (one-sided) -
##  balanced panel
## 
## data:  formula
## z = 57.914, p-value &lt; 2.2e-16
## alternative hypothesis: random effects sub AR(1) errors</code></pre>
</div>
<div id="conditional-lm-test-for-ar1-or-ma1-errors-under-random-effects" class="section level3">
<h3>Conditional LM test for AR(1) or MA(1) errors under random effects</h3>
<p><span class="citation">Baltagi and Li (1991)</span> and <span class="citation">Baltagi and Li (1995)</span> derive a Lagrange multiplier test for serial correlation in the idiosyncratic component of the errors under (normal, heteroskedastic) random effects. Under the null of serially uncorrelated errors, the test turns out to be identical for both the alternative of AR(1) and MA(1) processes. One- and two-sided versions are provided, the one-sided having power against positive serial correlation only. The two-sided is the default, while for the other one must specify the <code>alternative</code> option to <code>&quot;onesided&quot;</code>:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true"></a><span class="kw">pbltest</span>(<span class="kw">log</span>(gsp)<span class="op">~</span><span class="kw">log</span>(pcap)<span class="op">+</span><span class="kw">log</span>(pc)<span class="op">+</span><span class="kw">log</span>(emp)<span class="op">+</span>unemp, </span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true"></a>        <span class="dt">data=</span>Produc, <span class="dt">alternative=</span><span class="st">&quot;onesided&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Baltagi and Li one-sided LM test
## 
## data:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
## z = 21.69, p-value &lt; 2.2e-16
## alternative hypothesis: AR(1)/MA(1) errors in RE panel model</code></pre>
<p>As usual, the LM test statistic is based on residuals from the maximum likelihood estimate of the restricted model (random effects with serially uncorrelated errors). In this case, though, the restricted model cannot be estimated by OLS anymore, therefore the testing function depends on <code>lme()</code> in the <code>nlme</code> package for estimation of a random effects model by maximum likelihood. For this reason, the test is applicable only to balanced panels.</p>
<p>No test has been implemented to date for the symmetric hypothesis of no random effects in a model with errors following an AR(1) process, but an asymptotically equivalent likelihood ratio test is available in the <code>nlme</code> package (see Section <a href="#nlme">plm versus nlme and lme4</a>).</p>
</div>
<div id="general-serial-correlation-tests" class="section level3">
<h3>General serial correlation tests</h3>
<p>A general testing procedure for serial correlation in fixed effects (FE), random effects (RE) and pooled-OLS panel models alike can be based on considerations in <span class="citation">Wooldridge (2002)</span>, 10.7.2.</p>
<p>Recall that <code>plm</code> model objects are the result of OLS estimation performed on “demeaned” data, where, in the case of individual effects (else symmetric), this means time-demeaning for the FE (<code>within</code>) model, quasi-time-demeaning for the RE (<code>random</code>) model and original data, with no demeaning at all, for the pooled OLS (<code>pooling</code>) model (see Section <a href="#software-approach">software approach</a>).</p>
<p>For the random effects model, <span class="citation">Wooldridge (2002)</span> observes that under the null of homoskedasticity and no serial correlation in the idiosyncratic errors, the residuals from the quasi-demeaned regression must be spherical as well. Else, as the individual effects are wiped out in the demeaning, any remaining serial correlation must be due to the idiosyncratic component. Hence, a simple way of testing for serial correlation is to apply a standard serial correlation test to the quasi-demeaned model. The same applies in a pooled model, w.r.t. the original data.</p>
<p>The FE case needs some qualification. It is well-known that if the original model’s errors are uncorrelated then FE residuals are negatively serially correlated, with <span class="math inline">\(cor(\hat{u}_{it}, \hat{u}_{is})=-1/(T-1)\)</span> for each <span class="math inline">\(t,s\)</span> (see <span class="citation">Wooldridge (2002)</span> 10.5.4). This correlation clearly dies out as T increases, so this kind of AR test is applicable to <code>within</code> model objects only for T “sufficiently large”<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a>. On the converse, in short panels the test gets severely biased towards rejection (or, as the induced correlation is negative, towards acceptance in the case of the one-sided DW test with <code>alternative=&quot;greater&quot;</code>). See below for a serial correlation test applicable to “short” FE panel models.</p>
<p><code>plm</code> objects retain the “demeaned” data, so the procedure is straightforward for them. The wrapper functions <code>pbgtest</code> and <code>pdwtest</code> re-estimate the relevant quasi-demeaned model by OLS and apply, respectively, standard Breusch-Godfrey and Durbin-Watson tests from package <code>lmtest</code>:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true"></a><span class="kw">pbgtest</span>(grun.fe, <span class="dt">order =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## 
##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models
## 
## data:  inv ~ value + capital
## chisq = 42.587, df = 2, p-value = 5.655e-10
## alternative hypothesis: serial correlation in idiosyncratic errors</code></pre>
<p>The tests share the features of their OLS counterparts, in particular the <code>pbgtest</code> allows testing for higher-order serial correlation, which might turn useful, e.g., on quarterly data. Analogously, from the point of view of software, as the functions are simple wrappers towards <code>bgtest</code> and <code>dwtest</code>, all arguments from the latter two apply and may be passed on through the ellipsis (the <code>...</code> argument).</p>
</div>
<div id="wooldridges-test-for-serial-correlation-in-short-fe-panels" class="section level3">
<h3>Wooldridge’s test for serial correlation in “short” FE panels</h3>
<p>For the reasons reported above, under the null of no serial correlation in the errors, the residuals of a FE model must be negatively serially correlated, with <span class="math inline">\(cor(\hat{\epsilon}_{it}, \hat{\epsilon}_{is})=-1/(T-1)\)</span> for each <span class="math inline">\(t,s\)</span>. Wooldridge suggests basing a test for this null hypothesis on a pooled regression of FE residuals on themselves, lagged one period: <span class="math display">\[\hat{\epsilon}_{i,t}=\alpha + \delta
\hat{\epsilon}_{i,t-1} + \eta_{i,t}\]</span> Rejecting the restriction <span class="math inline">\(\delta = -1/(T-1)\)</span> makes us conclude against the original null of no serial correlation.</p>
<p>The building blocks available in <code>plm</code> make it easy to construct a function carrying out this procedure: first the FE model is estimated and the residuals retrieved, then they are lagged and a <code>pooling</code> AR(1) model is estimated. The test statistic is obtained by applying the above restriction on <span class="math inline">\(\delta\)</span> and supplying a heteroskedasticity- and autocorrelation-consistent covariance matrix (<code>vcovHC</code> with the appropriate options, in particular <code>method=&quot;arellano&quot;</code>)<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a>.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true"></a><span class="kw">pwartest</span>(<span class="kw">log</span>(emp) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(wage) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(capital), <span class="dt">data=</span>EmplUK)</span></code></pre></div>
<pre><code>## 
##  Wooldridge&#39;s test for serial correlation in FE panels
## 
## data:  plm.model
## F = 312.3, df1 = 1, df2 = 889, p-value &lt; 2.2e-16
## alternative hypothesis: serial correlation</code></pre>
<p>The test is applicable to any FE panel model, and in particular to “short” panels with small <span class="math inline">\(T\)</span> and large <span class="math inline">\(n\)</span>.</p>
</div>
<div id="wooldridges-first-difference-based-test" class="section level3">
<h3>Wooldridge’s first-difference-based test</h3>
<p>In the context of the first difference model, <span class="citation">Wooldridge (2002)</span>, 10.6.3 proposes a serial correlation test that can also be seen as a specification test to choose the most efficient estimator between fixed effects (<code>within</code>) and first difference (<code>fd</code>).</p>
<p>The starting point is the observation that if the idiosyncratic errors of the original model <span class="math inline">\(u_{it}\)</span> are uncorrelated, the errors of the (first) differenced model<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> <span class="math inline">\(e_{it} \equiv u_{it}-u_{i,t-1}\)</span> will be correlated, with <span class="math inline">\(cor(e_{it}, e_{i,t-1})=-0.5\)</span>, while any time-invariant effect, “fixed” or “random”, is wiped out in the differencing. So a serial correlation test for models with individual effects of any kind can be based on estimating the model <span class="math display">\[\hat{u}_{i,t}= \delta \hat{u}_{i,t-1} +
\eta_{i,t}\]</span> and testing the restriction <span class="math inline">\(\delta = -0.5\)</span>, corresponding to the null of no serial correlation. <span class="citation">Drukker (2003)</span> provides Monte Carlo evidence of the good empirical properties of the test.</p>
<p>On the other extreme (see <span class="citation">Wooldridge (2002)</span> 10.6.1), if the differenced errors <span class="math inline">\(e_{it}\)</span> are uncorrelated, as by definition <span class="math inline">\(u_{it} = u_{i,t-1} + e_{it}\)</span>, then <span class="math inline">\(u_{it}\)</span> is a random walk. In this latter case, the most efficient estimator is the first difference (<code>fd</code>) one; in the former case, it is the fixed effects one (<code>within</code>).</p>
<p>The function <code>pwfdtest</code> allows testing either hypothesis: the default behaviour <code>h0=&quot;fd&quot;</code> is to test for serial correlation in <em>first-differenced</em> errors:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true"></a><span class="kw">pwfdtest</span>(<span class="kw">log</span>(emp) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(wage) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(capital), <span class="dt">data=</span>EmplUK)</span></code></pre></div>
<pre><code>## 
##  Wooldridge&#39;s first-difference test for serial correlation in panels
## 
## data:  plm.model
## F = 1.5251, df1 = 1, df2 = 749, p-value = 0.2172
## alternative hypothesis: serial correlation in differenced errors</code></pre>
<p>while specifying <code>h0=&quot;fe&quot;</code> the null hypothesis becomes no serial correlation in <em>original</em> errors, which is similar to the <code>pwartest</code>.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true"></a><span class="kw">pwfdtest</span>(<span class="kw">log</span>(emp) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(wage) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(capital), <span class="dt">data=</span>EmplUK, <span class="dt">h0=</span><span class="st">&quot;fe&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Wooldridge&#39;s first-difference test for serial correlation in panels
## 
## data:  plm.model
## F = 131.55, df1 = 1, df2 = 749, p-value &lt; 2.2e-16
## alternative hypothesis: serial correlation in original errors</code></pre>
<p>Not rejecting one of the two is evidence in favour of using the estimator corresponding to <code>h0</code>. Should the truth lie in the middle (both rejected), whichever estimator is chosen will have serially correlated errors: therefore it will be advisable to use the autocorrelation-robust covariance estimators from the subsection <a href="#robust">robust covariance matrix estimation</a> in inference.</p>
</div>
</div>
<div id="tests-for-cross-sectional-dependence" class="section level2">
<h2>Tests for cross-sectional dependence</h2>
<p>Next to the more familiar issue of serial correlation, over the last years a growing body of literature has been dealing with cross-sectional dependence (henceforth: XSD) in panels, which can arise, e.g., if individuals respond to common shocks (as in the literature on <em>factor models</em>) or if spatial diffusion processes are present, relating individuals in a way depending on a measure of distance (<em>spatial models</em>).</p>
<p>The subject is huge, and here we touch only some general aspects of misspecification testing and valid inference. If XSD is present, the consequence is, at a minimum, inefficiency of the usual estimators and invalid inference when using the standard covariance matrix<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>. The plan is to have in <code>plm</code> both misspecification tests to detect XSD and robust covariance matrices to perform valid inference in its presence, like in the serial dependence case. For now, though, only misspecification tests are included.</p>
<div id="cd-and-lm-type-tests-for-global-cross-sectional-dependence" class="section level3">
<h3>CD and LM-type tests for global cross-sectional dependence</h3>
<p>The function <code>pcdtest</code> implements a family of XSD tests which can be applied in different settings, ranging from those where <span class="math inline">\(T\)</span> grows large with <span class="math inline">\(n\)</span> fixed to “short” panels with a big <span class="math inline">\(n\)</span> dimension and a few time periods. All are based on (transformations of–) the product-moment correlation coefficient of a model’s residuals, defined as</p>
<p><span class="math display">\[ \hat{\rho}_{ij}=\frac{\sum_{t=1}^T \hat{u}_{it} \hat{u}_{jt}}{(\sum_{t=1}^T \hat{u}^2_{it})^{1/2} (\sum_{t=1}^T \hat{u}^2_{jt})^{1/2} } \]</span></p>
<p>i.e., as averages over the time dimension of pairwise correlation coefficients for each pair of cross-sectional units.</p>
<p>The Breusch-Pagan <span class="citation">(Breusch and Pagan 1980)</span> LM test, based on the squares of <span class="math inline">\(\rho_{ij}\)</span>, is valid for <span class="math inline">\(T \rightarrow \infty\)</span> with <span class="math inline">\(n\)</span> fixed; defined as</p>
<p><span class="math display">\[LM=\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} T_{ij} \hat{\rho}_{ij}^2\]</span></p>
<p>where in the case of an unbalanced panel only pairwise complete observations are considered, and <span class="math inline">\(T_{ij}=min(T_i,T_j)\)</span> with <span class="math inline">\(T_i\)</span> being the number of observations for individual <span class="math inline">\(i\)</span>; else, if the panel is balanced, <span class="math inline">\(T_{ij}=T\)</span> for each <span class="math inline">\(i,j\)</span>. The test is distributed as <span class="math inline">\(\chi^2_{n(n-1)/2}\)</span>. It is inappropriate whenever the <span class="math inline">\(n\)</span> dimension is “large”. A scaled version, applicable also if <span class="math inline">\(T \rightarrow \infty\)</span> and <em>then</em> <span class="math inline">\(n \rightarrow \infty\)</span> (as in some pooled time series contexts), is defined as</p>
<p><span class="math display">\[SCLM=\sqrt{\frac{1}{n(n-1)}} ( \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} T_{ij} \hat{\rho}_{ij}^2 -1 )\]</span></p>
<p>and distributed as a standard normal (see <span class="citation">Pesaran (2004)</span>).</p>
<p>A bias-corrected scaled version, <span class="math inline">\(BCSCLM\)</span>, for the <em>fixed effect model with individual effects</em> only is also available which is simply the <span class="math inline">\(SCLM\)</span> with a term correcting for the bias (<span class="citation">Baltagi, Feng, and Kao (2012)</span>)<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a>. This statistic is also asymptotically distributed as standard normal. <span class="math display">\[BCSCLM=\sqrt{\frac{1}{n(n-1)}} ( \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} T_{ij} \hat{\rho}_{ij}^2 -1)-\frac{n}{2(T-1)}\]</span></p>
<p>Pesaran’s (<span class="citation">Pesaran (2004)</span>, <span class="citation">Pesaran (2015)</span>) <span class="math inline">\(CD\)</span> test</p>
<p><span class="math display">\[CD=\sqrt{\frac{2}{n(n-1)}} ( \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \sqrt{T_{ij}} \hat{\rho}_{ij} )\]</span></p>
<p>based on <span class="math inline">\(\rho_{ij}\)</span> without squaring (also distributed as a standard normal) is appropriate both in <span class="math inline">\(n\)</span>– and in <span class="math inline">\(T\)</span>–asymptotic settings. It has remarkable properties in samples of any practically relevant size and is robust to a variety of settings. The only big drawback is that the test loses power against the alternative of cross-sectional dependence if the latter is due to a factor structure with factor loadings averaging zero, that is, some units react positively to common shocks, others negatively.</p>
<p>The default version of the test is <code>&quot;cd&quot;</code> yielding Pesaran’s <span class="math inline">\(CD\)</span> test. These tests are originally meant to use the residuals of separate estimation of one time-series regression for each cross-sectional unit, so this is the default behaviour of <code>pcdtest</code>.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true"></a><span class="kw">pcdtest</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld)</span></code></pre></div>
<pre><code>## 
##  Pesaran CD test for cross-sectional dependence in panels
## 
## data:  inv ~ value + capital
## z = 5.3401, p-value = 9.292e-08
## alternative hypothesis: cross-sectional dependence</code></pre>
<p>If a different model specification (<code>within</code>, <code>random</code>, …) is assumed consistent, one can resort to its residuals for testing<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> by specifying the relevant <code>model</code> type. The main argument of this function may be either a model of class <code>panelmodel</code> or a <code>formula</code> and a <code>data.frame</code>; in the second case, unless <code>model</code> is set to <code>NULL</code>, all usual parameters relative to the estimation of a <code>plm</code> model may be passed on. The test is compatible with any consistent <code>panelmodel</code> for the data at hand, with any specification of <code>effect</code>. E.g., specifying <code>effect = &quot;time&quot;</code> or <code>effect = &quot;twoways&quot;</code> allows to test for residual cross-sectional dependence after the introduction of time fixed effects to account for common shocks.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true"></a><span class="kw">pcdtest</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Pesaran CD test for cross-sectional dependence in panels
## 
## data:  inv ~ value + capital
## z = 4.6612, p-value = 3.144e-06
## alternative hypothesis: cross-sectional dependence</code></pre>
<p>If the time dimension is insufficient and <code>model=NULL</code>, the function defaults to estimation of a <code>within</code> model and issues a warning.</p>
</div>
<div id="cdp-test-for-local-cross-sectional-dependence" class="section level3">
<h3>CD(p) test for local cross-sectional dependence</h3>
<p>A <em>local</em> variant of the <span class="math inline">\(CD\)</span> test, called <span class="math inline">\(CD(p)\)</span> test <span class="citation">(Pesaran 2004)</span>, takes into account an appropriate subset of <em>neighbouring</em> cross-sectional units to check the null of no XSD against the alternative of <em>local</em> XSD, i.e. dependence between neighbours only. To do so, the pairs of neighbouring units are selected by means of a binary proximity matrix like those used in spatial models. In the original paper, a regular ordering of observations is assumed, so that the <span class="math inline">\(m\)</span>-th cross-sectional observation is a neighbour to the <span class="math inline">\((m-1)\)</span>-th and to the <span class="math inline">\((m+1)\)</span>-th. Extending the <span class="math inline">\(CD(p)\)</span> test to irregular lattices, we employ the binary proximity matrix as a selector for discarding the correlation coefficients relative to pairs of observations that are not neighbours in computing the <span class="math inline">\(CD\)</span> statistic. The test is then defined as</p>
<p><span class="math display">\[CD=\sqrt{\frac{1}{\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w(p)_{ij}}} ( \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} [w(p)]_{ij} \sqrt{T_{ij}}\hat{\rho}_{ij} )\]</span></p>
<p>where <span class="math inline">\([w(p)]_{ij}\)</span> is the <span class="math inline">\((i,j)\)</span>-th element of the <span class="math inline">\(p\)</span>-th order proximity matrix, so that if <span class="math inline">\(h,k\)</span> are not neighbours, <span class="math inline">\([w(p)]_{hk}=0\)</span> and <span class="math inline">\(\hat{\rho}_{hk}\)</span> gets “killed”; this is easily seen to reduce to formula (14) in Pesaran <span class="citation">(Pesaran 2004)</span> for the special case considered in that paper. The same can be applied to the <span class="math inline">\(LM\)</span>, <span class="math inline">\(SCLM\)</span>, and <span class="math inline">\(BCSCLM\)</span> tests.</p>
<p>Therefore, the <em>local</em> version of either test can be computed supplying an <span class="math inline">\(n \times n\)</span> matrix (of any kind coercible to <code>logical</code>), providing information on whether any pair of observations are neighbours or not, to the <code>w</code> argument. If <code>w</code> is supplied, only neighbouring pairs will be used in computing the test; else, <code>w</code> will default to <code>NULL</code> and all observations will be used. The matrix needs not really be binary, so commonly used “row-standardized” matrices can be employed as well: it is enough that neighbouring pairs correspond to nonzero elements in <code>w</code>.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
</div>
</div>
<div id="unit-root-tests" class="section level2">
<h2>Unit root tests</h2>
<div id="preliminary-results" class="section level3">
<h3>Preliminary results</h3>
<p>We consider the following model:</p>
<p><span class="math display">\[
y_{it} = \delta y_{it-1} + \sum_{L=1}^{p_i} \theta_i \Delta
y_{it-L}+\alpha_{mi} d_{mt}+\epsilon_{it}
\]</span></p>
<p>The unit root hypothesis is <span class="math inline">\(\rho = 1\)</span>. The model can be rewritten in difference:</p>
<p><span class="math display">\[
\Delta y_{it} = \rho y_{it-1} + \sum_{L=1}^{p_i} \theta_i \Delta
y_{it-L}+\alpha_{mi} d_{mt}+\epsilon_{it}
\]</span></p>
<p>So that the unit-root hypothesis is now <span class="math inline">\(\rho = 0\)</span>.</p>
<p>Some of the unit-root tests for panel data are based on preliminary results obtained by running the above Augmented Dickey-Fuller (ADF) regression.</p>
<p>First, we have to determine the optimal number of lags <span class="math inline">\(p_i\)</span> for each time-series. Several possibilities are available. They all have in common that the maximum number of lags have to be chosen first. Then, <span class="math inline">\(p_i\)</span> can be chosen by using:</p>
<ul>
<li>the Schwarz information criterion (SIC),</li>
<li>the Akaike information criterion (AIC),</li>
<li>the Hall’s method, which consist in removing the higher lags while they are not significant.</li>
</ul>
<p>The ADF regression is run on <span class="math inline">\(T-p_i-1\)</span> observations for each individual, so that the total number of observations is <span class="math inline">\(n\times \tilde{T}\)</span> where <span class="math inline">\(\tilde{T}=T-p_i-1\)</span></p>
<p><span class="math inline">\(\bar{p}\)</span> is the average number of lags. Call <span class="math inline">\(e_{i}\)</span> the vector of residuals.</p>
<p>Estimate the variance of the <span class="math inline">\(\epsilon_i\)</span> as:</p>
<p><span class="math display">\[
\hat{\sigma}_{\epsilon_i}^2 = \frac{\sum_{t=p_i+1}^{T} e_{it}^2}{df_i}
\]</span></p>
</div>
<div id="levin-lin-chu-model" class="section level3">
<h3>Levin-Lin-Chu model</h3>
<p>Then, compute artificial regressions of <span class="math inline">\(\Delta y_{it}\)</span> and <span class="math inline">\(y_{it-1}\)</span> on <span class="math inline">\(\Delta y_{it-L}\)</span> and <span class="math inline">\(d_{mt}\)</span> and get the two vectors of residuals <span class="math inline">\(z_{it}\)</span> and <span class="math inline">\(v_{it}\)</span>.</p>
<p>Standardize these two residuals and run the pooled regression of <span class="math inline">\(z_{it}/\hat{\sigma}_i\)</span> on <span class="math inline">\(v_{it}/\hat{\sigma}_i\)</span> to get <span class="math inline">\(\hat{\rho}\)</span>, its standard deviation <span class="math inline">\(\hat{\sigma}({\hat{\rho}})\)</span> and the t-statistic <span class="math inline">\(t_{\hat{\rho}}=\hat{\rho}/\hat{\sigma}({\hat{\rho}})\)</span>.</p>
<p>Compute the long run variance of <span class="math inline">\(y_i\)</span> :</p>
<p><span class="math display">\[
\hat{\sigma}_{yi}^2 = \frac{1}{T-1}\sum_{t=2}^T \Delta y_{it}^2 + 2
\sum_{L=1}^{\bar{K}}w_{\bar{K}L}\left[\frac{1}{T-1}\sum_{t=2+L}^T
  \Delta y_{it} \Delta y_{it-L}\right]
\]</span></p>
<p>Define <span class="math inline">\(\bar{s}_i\)</span> as the ratio of the long and short term variance and <span class="math inline">\(\bar{s}\)</span> the mean for all the individuals of the sample</p>
<p><span class="math display">\[
s_i = \frac{\hat{\sigma}_{yi}}{\hat{\sigma}_{\epsilon_i}}
\]</span></p>
<p><span class="math display">\[
\bar{s} = \frac{\sum_{i=1}^n s_i}{n}
\]</span></p>
<p><span class="math display">\[
t^*_{\rho}=\frac{t_{\rho}- n \bar{T} \bar{s} \hat{\sigma}_{\tilde{\epsilon}}^{-2}
\hat{\sigma}({\hat{\rho}}) \mu^*_{m\tilde{T}}}{\sigma^*_{m\tilde{T}}}
\]</span></p>
<p>follows a normal distribution under the null hypothesis of stationarity. <span class="math inline">\(\mu^*_{m\tilde{T}}\)</span> and <span class="math inline">\(\sigma^*_{m\tilde{T}}\)</span> are given in table 2 of the original paper and are also available in the package.</p>
</div>
<div id="im-pesaran-and-shin-test" class="section level3">
<h3>Im, Pesaran and Shin test</h3>
<p>This test does not require that <span class="math inline">\(\rho\)</span> is the same for all the individuals. The null hypothesis is still that all the series have an unit root, but the alternative is that some may have a unit root and others have different values of <span class="math inline">\(\rho_i &lt;0\)</span>.</p>
<p>The test is based on the average of the student statistic of the <span class="math inline">\(\rho\)</span> obtained for each individual:</p>
<p><span class="math display">\[
\bar{t}=\frac{1}{n}\sum_{i=1}^n t_{\rho i}
\]</span></p>
<p>The statistic is then:</p>
<p><span class="math display">\[
z = \frac{\sqrt{n}\left(\bar{t}- E(\bar{t})\right)}{\sqrt{V(\bar{t})}}
\]</span></p>
<p><span class="math inline">\(\mu^*_{m\tilde{T}}\)</span> and <span class="math inline">\(\sigma^*_{m\tilde{T}}\)</span> are given in table 2 of the original paper and are also available in the package.</p>
</div>
</div>
<div id="robust" class="section level2">
<h2>Robust covariance matrix estimation</h2>
<p>Robust estimators of the covariance matrix of coefficients are provided, mostly for use in Wald-type tests. <code>vcovHC</code> estimates three “flavours” of White’s heteroskedasticity-consistent covariance matrix<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> (known as the <em>sandwich</em> estimator). Interestingly, in the context of panel data the most general version also proves consistent vs. serial correlation.</p>
<p>All types assume no correlation between errors of different groups while allowing for heteroskedasticity across groups, so that the full covariance matrix of errors is <span class="math inline">\(V=I_n \otimes \Omega_i; i=1,..,n\)</span>. As for the <em>intragroup</em> error covariance matrix of every single group of observations, <code>&quot;white1&quot;</code> allows for general heteroskedasticity but no serial correlation, <em>i.e.</em></p>
<p><span class="math display">\[\begin{equation}
 (\#eq:omegaW1)
 \Omega_i=
 \left[ \begin{array}{c c c c}
 \sigma_{i1}^2 &amp; \dots &amp; \dots &amp; 0 \\
 0 &amp; \sigma_{i2}^2 &amp; &amp; \vdots \\
 \vdots &amp; &amp; \ddots &amp; 0 \\
 0 &amp; ... &amp; ... &amp; \sigma_{iT}^2 \\
 \end{array} \right]
\end{equation}\]</span></p>
<p>while <code>&quot;white2&quot;</code> is <code>&quot;white1&quot;</code> restricted to a common variance inside every group, estimated as <span class="math inline">\(\sigma_i^2=\sum_{t=1}^T{\hat{u}_{it}^2}/T\)</span>, so that <span class="math inline">\(\Omega_i=I_T \otimes \sigma_i^2\)</span> (see <span class="citation">Greene (2003)</span>, 13.7.1–2 and <span class="citation">Wooldridge (2002)</span>, 10.7.2; <code>&quot;arellano&quot;</code> (see ibid. and the original ref. <span class="citation">Arellano (1987)</span>) allows a fully general structure w.r.t. heteroskedasticity and serial correlation:</p>
<p><span class="math display">\[\begin{equation}
 (\#eq:omegaArellano)
 \Omega_i=
 \left[ \begin{array}{c c c c c}
 \sigma_{i1}^2 &amp; \sigma_{i1,i2}  &amp; \dots &amp; \dots &amp; \sigma_{i1,iT} \\
 \sigma_{i2,i1} &amp; \sigma_{i2}^2 &amp; &amp; &amp; \vdots \\
 \vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
 \vdots &amp; &amp; &amp; \sigma_{iT-1}^2 &amp; \sigma_{iT-1,iT} \\
 \sigma_{iT,i1} &amp; \dots &amp; \dots &amp; \sigma_{iT,iT-1} &amp; \sigma_{iT}^2 \\
 \end{array} \right]
\end{equation}\]</span></p>
<p>The latter is, as already observed, consistent w.r.t. timewise correlation of the errors, but on the converse, unlike the White 1 and 2 methods, it relies on large <span class="math inline">\(n\)</span> asymptotics with small <span class="math inline">\(T\)</span>.</p>
<p>The fixed effects case, as already observed in Section <a href="#serialcor">tests of serial correlation</a> on serial correlation, is complicated by the fact that the demeaning induces serial correlation in the errors. The original White estimator (<code>white1</code>) turns out to be inconsistent for fixed <span class="math inline">\(T\)</span> as <span class="math inline">\(n\)</span> grows, so in this case it is advisable to use the <code>arellano</code> version (see <span class="citation">Stock and Watson (2008)</span>).</p>
<p>The errors may be weighted according to the schemes proposed by <span class="citation">MacKinnon and White (1985)</span> and <span class="citation">Cribari–Neto (2004)</span> to improve small-sample performance<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>.</p>
<p>The main use of <code>vcovHC</code> is together with testing functions from the <code>lmtest</code> and <code>car</code> packages. These typically allow passing the <code>vcov</code> parameter either as a matrix or as a function (see <span class="citation">Zeileis (2004)</span>). If one is happy with the defaults, it is easiest to pass the function itself<a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a>:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;lmtest&quot;</span>)</span></code></pre></div>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true"></a>re &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;random&quot;</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true"></a><span class="kw">coeftest</span>(re, vcovHC, <span class="dt">df =</span> <span class="ot">Inf</span>)</span></code></pre></div>
<pre><code>## 
## z test of coefficients:
## 
##               Estimate Std. Error z value  Pr(&gt;|z|)    
## (Intercept) -57.834415  23.449626 -2.4663   0.01365 *  
## value         0.109781   0.012984  8.4551 &lt; 2.2e-16 ***
## capital       0.308113   0.051889  5.9379 2.887e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>else one may do the covariance computation inside the call to <code>coeftest</code>, thus passing on a matrix:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true"></a><span class="kw">coeftest</span>(re, <span class="kw">vcovHC</span>(re, <span class="dt">method=</span><span class="st">&quot;white2&quot;</span>, <span class="dt">type=</span><span class="st">&quot;HC3&quot;</span>), <span class="dt">df =</span> <span class="ot">Inf</span>)</span></code></pre></div>
<p>For some tests, e.g., for multiple model comparisons by <code>waldtest</code>, one should always provide a function<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>. In this case, optional parameters are provided as shown below (see also <span class="citation">Zeileis (2004)</span>, p. 12):</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true"></a><span class="kw">waldtest</span>(re, <span class="kw">update</span>(re,.<span class="op">~</span>.<span class="op">-</span>capital),</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true"></a>         <span class="dt">vcov=</span><span class="cf">function</span>(x) <span class="kw">vcovHC</span>(x, <span class="dt">method=</span><span class="st">&quot;white2&quot;</span>, <span class="dt">type=</span><span class="st">&quot;HC3&quot;</span>))</span></code></pre></div>
<pre><code>## Wald test
## 
## Model 1: inv ~ value + capital
## Model 2: inv ~ value
##   Res.Df Df  Chisq Pr(&gt;Chisq)    
## 1    197                         
## 2    198 -1 87.828  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Moreover, <code>linearHypothesis</code> from package <code>car</code> may be used to test for linear restrictions:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true"></a><span class="kw">library</span>(<span class="st">&quot;car&quot;</span>)</span></code></pre></div>
<pre><code>## Loading required package: carData</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true"></a><span class="kw">linearHypothesis</span>(re, <span class="st">&quot;2*value=capital&quot;</span>, <span class="dt">vcov.=</span>vcovHC)</span></code></pre></div>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## 2 value - capital = 0
## 
## Model 1: restricted model
## Model 2: inv ~ value + capital
## 
## Note: Coefficient covariance matrix supplied.
## 
##   Res.Df Df  Chisq Pr(&gt;Chisq)  
## 1    198                       
## 2    197  1 3.4783    0.06218 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>A specific <code>vcovHC</code> method for <code>pgmm</code> objects is also provided which implements the robust covariance matrix proposed by <span class="citation">Windmeijer (2005)</span> for generalized method of moments estimators.</p>
</div>
</div>
<div id="nlme" class="section level1">
<h1>plm versus nlme and lme4</h1>
<p>The models termed <em>panel</em> by the econometricians have counterparts in the statistics literature on <em>mixed</em> models (or <em>hierarchical models</em>, or <em>models for longitudinal data</em>), although there are both differences in jargon and more substantial distinctions. This language inconsistency between the two communities, together with the more complicated general structure of statistical models for longitudinal data and the associated notation in the software, is likely to scare some practicing econometricians away from some potentially useful features of the <code>R</code> environment, so it may be useful to provide here a brief reconciliation between the typical panel data specifications used in econometrics and the general framework used in statistics for mixed models<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>.</p>
<p><code>R</code> is particularly strong on mixed models’ estimation, thanks to the long-standing <code>nlme</code> package (see <span class="citation">Pinheiro et al. (2007)</span>) and the more recent <code>lme4</code> package, based on S4 classes (see <span class="citation">Bates (2007)</span>)<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>. In the following we will refer to the more established <code>nlme</code> to give some examples of “econometric” panel models that can be estimated in a likelihood framework, also including some likelihood ratio tests. Some of them are not feasible in <code>plm</code> and make a useful complement to the econometric “toolbox” available in <code>R</code>.</p>
<div id="fundamental-differences-between-the-two-approaches" class="section level2">
<h2>Fundamental differences between the two approaches</h2>
<p>Econometrics deal mostly with non-experimental data. Great emphasis is put on specification procedures and misspecification testing. Model specifications tend therefore to be very simple, while great attention is put on the issues of endogeneity of the regressors, dependence structures in the errors and robustness of the estimators under deviations from normality. The preferred approach is often semi- or non-parametric, and heteroskedasticity-consistent techniques are becoming standard practice both in estimation and testing.</p>
<p>For all these reasons, although the maximum likelihood framework is important in testing<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> and sometimes used in estimation as well, panel model estimation in econometrics is mostly accomplished in the generalized least squares framework based on Aitken’s Theorem and, when possible, in its special case OLS, which are free from distributional assumptions (although these kick in at the diagnostic testing stage). On the contrary, longitudinal data models in <code>nlme</code> and <code>lme4</code> are estimated by (restricted or unrestricted) maximum likelihood. While under normality, homoskedasticity and no serial correlation of the errors OLS are also the maximum likelihood estimator, in all the other cases there are important differences.</p>
<p>The econometric GLS approach has closed-form analytical solutions computable by standard linear algebra and, although the latter can sometimes get computationally heavy on the machine, the expressions for the estimators are usually rather simple. ML estimation of longitudinal models, on the contrary, is based on numerical optimization of nonlinear functions without closed-form solutions and is thus dependent on approximations and convergence criteria. For example, the “GLS” functionality in <code>nlme</code> is rather different from its “econometric” counterpart. “Feasible GLS” estimation in <code>plm</code> is based on a single two-step procedure, in which an inefficient but consistent estimation method (typically OLS) is employed first in order to get a consistent estimate of the errors’ covariance matrix, to be used in GLS at the second step; on the converse, “GLS” estimators in <code>nlme</code> are based on iteration until convergence of two-step optimization of the relevant likelihood.</p>
</div>
<div id="some-false-friends" class="section level2">
<h2>Some false friends</h2>
<p>The <em>fixed/random effects</em> terminology in econometrics is often recognized to be misleading, as both are treated as random variates in modern econometrics (see, e.g., <span class="citation">Wooldridge (2002)</span> 10.2.1). It has been recognized since Mundlak’s classic paper (<span class="citation">Mundlak (1978)</span>) that the fundamental issue is whether the unobserved effects are correlated with the regressors or not. In this last case, they can safely be left in the error term, and the serial correlation they induce is cared for by means of appropriate GLS transformations. On the contrary, in the case of correlation, “fixed effects” methods such as least squares dummy variables or time-demeaning are needed, which explicitly, although inconsistently<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>, estimate a group– (or time–) invariant additional parameter for each group (or time period).</p>
<p>Thus, from the point of view of model specification, having <em>fixed effects</em> in an econometric model has the meaning of allowing the intercept to vary with group, or time, or both, while the other parameters are generally still assumed to be homogeneous. Having <em>random effects</em> means having a group– (or time–, or both) specific component in the error term.</p>
<p>In the mixed models literature, on the contrary, <em>fixed effect</em> indicates a parameter that is assumed constant, while <em>random effects</em> are parameters that vary randomly around zero according to a joint multivariate normal distribution.</p>
<!---
 So, the FE model in econometrics has no counterpart in the mixed
 models framework, unless reducing it to OLS on a specification with
 one dummy for each group (viz. time period, or both) (often termed
 *least squares dummy variables*, or LSDV model) which can
 trivially be estimated by OLS. 
-->
<p>So, the FE model in econometrics has no counterpart in the mixed models framework, unless reducing it to OLS on a specification with one dummy for each group (often termed <em>least squares dummy variables</em>, or LSDV model) which can trivially be estimated by OLS. The RE model is instead a special case of a mixed model where only the intercept is specified as a random effect, while the “random” type variable coefficients model can be seen as one that has the same regressors in the fixed and random sets. The unrestricted generalized least squares can in turn be seen, in the <code>nlme</code> framework, as a standard linear model with a general error covariance structure within the groups and errors uncorrelated across groups.</p>
</div>
<div id="a-common-taxonomy" class="section level2">
<h2>A common taxonomy</h2>
<p>To reconcile the two terminologies, in the following we report the specification of the panel models in <code>plm</code> according to the general expression of a mixed model in Laird-Ware form <span class="citation">(see the web appendix to Fox 2002)</span> and the <code>nlme</code> estimation commands for maximum likelihood estimation of an equivalent specification<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>.</p>
<div id="the-laird-ware-representation-for-mixed-models" class="section level3">
<h3>The Laird-Ware representation for mixed models</h3>
<p>A general representation for the linear mixed effects model is given in <span class="citation">Laird and Ware (1982)</span>.</p>
<p><span class="math display">\[
\begin{array}{rcl}
y_{it} &amp; = &amp; \beta_1 x_{1ij} +  \dots + \beta_p x_{pij} \\
 &amp;  &amp; b_1 z_{1ij} +  \dots + b_p z_{pij} + \epsilon_{ij} \\
b_{ik} &amp; \sim &amp; N(0,\psi^2_k), \phantom{p} Cov(b_k,b_{k&#39;}) = \psi_{kk&#39;} \\
\epsilon_{ij} &amp; \sim &amp; N(0,\sigma^2 \lambda_{ijj}), \phantom{p} Cov(\epsilon_{ij},\epsilon_{ij&#39;}) = \sigma^2 \lambda_{ijj&#39;} \\
\end{array}
\]</span></p>
<p>where the <span class="math inline">\(x_1, \dots x_p\)</span> are the fixed effects regressors and the <span class="math inline">\(z_1, \dots z_p\)</span> are the random effects regressors, assumed to be normally distributed across groups. The covariance of the random effects coefficients <span class="math inline">\(\psi_{kk&#39;}\)</span> is assumed constant across groups and the covariances between the errors in group <span class="math inline">\(i\)</span>, <span class="math inline">\(\sigma^2 \lambda_{ijj&#39;}\)</span>, are described by the term <span class="math inline">\(\lambda_{ijj&#39;}\)</span> representing the correlation structure of the errors within each group (e.g., serial correlation over time) scaled by the common error variance <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="pooling-and-within" class="section level3">
<h3>Pooling and Within</h3>
<p>The <em>pooling</em> specification in <code>plm</code> is equivalent to a classical linear model (i.e., no random effects regressor and spherical errors: <span class="math inline">\(b_{iq}=0 \phantom{p} \forall i,q, \phantom{p} \lambda_{ijj}=\sigma^2\)</span> for <span class="math inline">\(j=j&#39;\)</span>, <span class="math inline">\(0\)</span> else). The <em>within</em> one is the same with the regressors’ set augmented by <span class="math inline">\(n-1\)</span> group dummies. There is no point in using <code>nlme</code> as parameters can be estimated by OLS which is also ML.</p>
</div>
<div id="random-effects" class="section level3">
<h3>Random effects</h3>
<p>In the Laird and Ware notation, the RE specification is a model with only one random effects regressor: the intercept. Formally, <span class="math inline">\(z_{1ij}=1 \phantom{p}\forall i,j, \phantom{p} z_{qij}=0 \phantom{p} \forall i, \forall j, \forall q \neq 1\)</span> <span class="math inline">\(\lambda_{ij}=1\)</span> for <span class="math inline">\(i=j\)</span>, <span class="math inline">\(0\)</span> else). The composite error is therefore <span class="math inline">\(u_{ij}=1b_{i1} + \epsilon_{ij}\)</span>. Below we report coefficients of Grunfeld’s model estimated by GLS and then by ML:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true"></a><span class="kw">library</span>(nlme)</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true"></a>reGLS &lt;-<span class="st"> </span><span class="kw">plm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;random&quot;</span>)</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true"></a></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true"></a>reML &lt;-<span class="st"> </span><span class="kw">lme</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">random=</span><span class="op">~</span><span class="dv">1</span><span class="op">|</span>firm)</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true"></a></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true"></a><span class="kw">coef</span>(reGLS)</span></code></pre></div>
<pre><code>## (Intercept)       value     capital 
## -57.8344149   0.1097812   0.3081130</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true"></a><span class="kw">summary</span>(reML)<span class="op">$</span>coefficients<span class="op">$</span>fixed</span></code></pre></div>
<pre><code>## (Intercept)       value     capital 
## -57.8644245   0.1097897   0.3081881</code></pre>
</div>
<div id="variable-coefficients-random" class="section level3">
<h3>Variable coefficients, “random”</h3>
<p>Swamy’s variable coefficients model <span class="citation">(Swamy 1970)</span> has coefficients varying randomly (and independently of each other) around a set of fixed values, so the equivalent specification is <span class="math inline">\(z_{q}=x_{q} \phantom{p} \forall q\)</span>, i.e. the fixed effects and the random effects regressors are the same, and <span class="math inline">\(\psi_{kk&#39;}=\sigma_\mu^2 I_N\)</span>, and <span class="math inline">\(\lambda_{ijj}=1\)</span>, <span class="math inline">\(\lambda_{ijj&#39;}=0\)</span> for <span class="math inline">\(j \neq j&#39;\)</span>, that’s to say they are not correlated.</p>
<p>Estimation of a mixed model with random coefficients on all regressors is rather demanding from the computational side. Some models from our examples fail to converge. The below example is estimated on the Grunfeld data and model with time effects.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true"></a>vcm &lt;-<span class="st"> </span><span class="kw">pvcm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;random&quot;</span>, <span class="dt">effect=</span><span class="st">&quot;time&quot;</span>)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true"></a></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true"></a>vcmML &lt;-<span class="st"> </span><span class="kw">lme</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">random=</span><span class="op">~</span>value<span class="op">+</span>capital<span class="op">|</span>year)</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true"></a></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true"></a><span class="kw">coef</span>(vcm)</span></code></pre></div>
<pre><code>## (Intercept)       value     capital 
## -18.5538638   0.1239595   0.1114579</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true"></a><span class="kw">summary</span>(vcmML)<span class="op">$</span>coefficients<span class="op">$</span>fixed</span></code></pre></div>
<pre><code>## (Intercept)       value     capital 
## -26.3558395   0.1241982   0.1381782</code></pre>
</div>
<div id="variable-coefficients-within" class="section level3">
<h3>Variable coefficients, “within”</h3>
<p>This specification actually entails separate estimation of <span class="math inline">\(T\)</span> different standard linear models, one for each group in the data, so the estimation approach is the same: OLS. In <code>nlme</code> this is done by creating an <code>lmList</code> object, so that the two models below are equivalent (output suppressed):</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true"></a>vcmf &lt;-<span class="st"> </span><span class="kw">pvcm</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld, <span class="dt">model=</span><span class="st">&quot;within&quot;</span>, <span class="dt">effect=</span><span class="st">&quot;time&quot;</span>)</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true"></a></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true"></a>vcmfML &lt;-<span class="st"> </span><span class="kw">lmList</span>(inv<span class="op">~</span>value<span class="op">+</span>capital<span class="op">|</span>year, <span class="dt">data=</span>Grunfeld)</span></code></pre></div>
</div>
<div id="general-fgls" class="section level3">
<h3>General FGLS</h3>
<p>The general, or unrestricted, feasible GLS (FGLS), <code>pggls</code> in the <code>plm</code> nomenclature, is equivalent to a model with no random effects regressors (<span class="math inline">\(b_{iq}=0 \phantom{p} \forall i,q\)</span>) and an error covariance structure which is unrestricted within groups apart from the usual requirements. The function for estimating such models with correlation in the errors but no random effects is <code>gls()</code>.</p>
<p>This very general serial correlation and heteroskedasticity structure is not estimable for the original Grunfeld data, which have more time periods than firms, therefore we restrict them to firms 4 to 6.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true"></a>sGrunfeld &lt;-<span class="st"> </span>Grunfeld[Grunfeld<span class="op">$</span>firm <span class="op">%in%</span><span class="st"> </span><span class="dv">4</span><span class="op">:</span><span class="dv">6</span>, ]</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true"></a></span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true"></a>ggls &lt;-<span class="st"> </span><span class="kw">pggls</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>sGrunfeld, <span class="dt">model=</span><span class="st">&quot;pooling&quot;</span>)</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true"></a></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true"></a>gglsML &lt;-<span class="st"> </span><span class="kw">gls</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>sGrunfeld,</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true"></a>              <span class="dt">correlation=</span><span class="kw">corSymm</span>(<span class="dt">form=</span><span class="op">~</span><span class="dv">1</span><span class="op">|</span>year))</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true"></a></span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true"></a><span class="kw">coef</span>(ggls)</span></code></pre></div>
<pre><code>## (Intercept)       value     capital 
##  1.19679342  0.10555908  0.06600166</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true"></a><span class="kw">summary</span>(gglsML)<span class="op">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)       value     capital 
##  -2.4156266   0.1163550   0.0735837</code></pre>
<p>The <em>within</em> case is analogous, with the regressor set augmented by <span class="math inline">\(n-1\)</span> group dummies.</p>
</div>
</div>
<div id="some-useful-econometric-models-in-nlme" class="section level2">
<h2>Some useful “econometric” models in nlme</h2>
<p>Finally, amongst the many possible specifications estimable with <code>nlme</code>, we report a couple cases that might be especially interesting to applied econometricians.</p>
<div id="ar1-pooling-or-random-effects-panel" class="section level3">
<h3>AR(1) pooling or random effects panel</h3>
<p>Linear models with groupwise structures of time-dependence<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> may be fitted by <code>gls()</code>, specifying the correlation structure in the <code>correlation</code> option<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a>:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true"></a>Grunfeld<span class="op">$</span>year &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.character</span>(Grunfeld<span class="op">$</span>year))</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true"></a>lmAR1ML &lt;-<span class="st"> </span><span class="kw">gls</span>(inv<span class="op">~</span>value<span class="op">+</span>capital,<span class="dt">data=</span>Grunfeld,</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true"></a>               <span class="dt">correlation=</span><span class="kw">corAR1</span>(<span class="dv">0</span>,<span class="dt">form=</span><span class="op">~</span>year<span class="op">|</span>firm))</span></code></pre></div>
<p>and analogously the random effects panel with, e.g., AR(1) errors (see <span class="citation">Baltagi (2005)</span>, <span class="citation">Baltagi (2013)</span>, ch. 5), which is a very common specification in econometrics, may be fit by <code>lme</code> specifying an additional random intercept:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true"></a>reAR1ML &lt;-<span class="st"> </span><span class="kw">lme</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld,<span class="dt">random=</span><span class="op">~</span><span class="dv">1</span><span class="op">|</span>firm,</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true"></a>               <span class="dt">correlation=</span><span class="kw">corAR1</span>(<span class="dv">0</span>,<span class="dt">form=</span><span class="op">~</span>year<span class="op">|</span>firm))</span></code></pre></div>
<p>The regressors’ coefficients and the error’s serial correlation coefficient may be retrieved this way:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true"></a><span class="kw">summary</span>(reAR1ML)<span class="op">$</span>coefficients<span class="op">$</span>fixed</span></code></pre></div>
<pre><code>##  (Intercept)        value      capital 
## -40.27650822   0.09336672   0.31323330</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true"></a><span class="kw">coef</span>(reAR1ML<span class="op">$</span>modelStruct<span class="op">$</span>corStruct, <span class="dt">unconstrained=</span><span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>##      Phi 
## 0.823845</code></pre>
<p>Significance statistics for the regressors’ coefficients are to be found in the usual <code>summary</code> object, while to get the significance test of the serial correlation coefficient one can do a likelihood ratio test as shown in the following.</p>
</div>
<div id="an-lr-test-for-serial-correlation-and-one-for-random-effects" class="section level3">
<h3>An LR test for serial correlation and one for random effects</h3>
<p>A likelihood ratio test for serial correlation in the idiosyncratic residuals can be done as a nested models test, by <code>anova()</code>, comparing the model with spherical idiosyncratic residuals with the more general alternative featuring AR(1) residuals. The test takes the form of a zero restriction test on the autoregressive parameter.</p>
<p>This can be done on pooled or random effects models alike. First we report the simpler case.</p>
<p>We already estimated the pooling AR(1) model above. The GLS model without correlation in the residuals is the same as OLS, and one could well use <code>lm()</code> for the restricted model. Here we estimate it by <code>gls()</code>.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true"></a>lmML &lt;-<span class="st"> </span><span class="kw">gls</span>(inv<span class="op">~</span>value<span class="op">+</span>capital, <span class="dt">data=</span>Grunfeld)</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true"></a><span class="kw">anova</span>(lmML, lmAR1ML)</span></code></pre></div>
<pre><code>##         Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## lmML        1  4 2400.217 2413.350 -1196.109                        
## lmAR1ML     2  5 2094.936 2111.352 -1042.468 1 vs 2 307.2813  &lt;.0001</code></pre>
<p>The AR(1) test on the random effects model is to be done in much the same way, using the random effects model objects estimated above:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true"></a><span class="kw">anova</span>(reML, reAR1ML)</span></code></pre></div>
<pre><code>##         Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## reML        1  5 2205.851 2222.267 -1097.926                        
## reAR1ML     2  6 2094.802 2114.501 -1041.401 1 vs 2 113.0496  &lt;.0001</code></pre>
<p>A likelihood ratio test for random effects compares the specifications with and without random effects and spherical idiosyncratic errors:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true"></a><span class="kw">anova</span>(lmML, reML)</span></code></pre></div>
<pre><code>##      Model df      AIC      BIC    logLik   Test L.Ratio p-value
## lmML     1  4 2400.217 2413.350 -1196.109                       
## reML     2  5 2205.851 2222.267 -1097.926 1 vs 2 196.366  &lt;.0001</code></pre>
<p>The random effects, AR(1) errors model in turn nests the AR(1) pooling model, therefore a likelihood ratio test for random effects sub AR(1) errors may be carried out, again, by comparing the two autoregressive specifications:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true"></a><span class="kw">anova</span>(lmAR1ML, reAR1ML)</span></code></pre></div>
<pre><code>##         Model df      AIC      BIC    logLik   Test  L.Ratio p-value
## lmAR1ML     1  5 2094.936 2111.352 -1042.468                        
## reAR1ML     2  6 2094.802 2114.501 -1041.401 1 vs 2 2.134349   0.144</code></pre>
<p>whence we see that the Grunfeld model specification doesn’t seem to need any random effects once we control for serial correlation in the data.</p>
</div>
</div>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>With <code>plm</code> we aim at providing a comprehensive package containing the standard functionalities that are needed for the management and the econometric analysis of panel data. In particular, we provide: functions for data transformation; estimators for pooled, random and fixed effects static panel models and variable coefficients models, general GLS for general covariance structures, and generalized method of moments estimators for dynamic panels; specification and diagnostic tests. Instrumental variables estimation is supported. Most estimators allow working with unbalanced panels. While among the different approaches to longitudinal data analysis we take the perspective of the econometrician, the syntax is consistent with the basic linear modeling tools, like the <code>lm</code> function.</p>
<p>On the input side, <code>formula</code> and <code>data</code> arguments are used to specify the model to be estimated. Special functions are provided to make writing formulas easier, and the structure of the data is indicated with an <code>index</code> argument.</p>
<p>On the output side, the model objects (of the new class <code>panelmodel</code>) are compatible with the general restriction testing frameworks of packages <code>lmtest</code> and <code>car</code>. Specialized methods are also provided for the calculation of robust covariance matrices; heteroskedasticity- and correlation-consistent testing is accomplished by passing these on to testing functions, together with a <code>panelmodel</code> object.</p>
<p>The main functionalities of the package have been illustrated here by applying them on some well-known data sets from the econometric literature. The similarities and differences with the maximum likelihood approach to longitudinal data have also been briefly discussed.</p>
<!--
We plan to expand the methods in this paper to systems of equations
and to the estimation and testing of models with autoregressive
errors. Additions to the misspecification testing toolbox and
covariance estimators robust vs. cross-sectional correlation are also
in the offing. Lastly, conditional visualization features in the
`R` environment seem to offer a promising toolbox for visual
diagnostics, which is another subject for future work.
-->
<p>We plan to expand the methods in this paper to systems of equations and to the estimation of models with autoregressive errors. Addition of covariance estimators robust vs. cross-sectional correlation are also in the offing. Lastly, conditional visualization features in the <code>R</code> environment seem to offer a promising toolbox for visual diagnostics, which is another subject for future work.</p>
</div>
<div id="acknowledgments" class="section level1 unnumbered">
<h1 class="unnumbered">Acknowledgments</h1>
<p>While retaining responsibility for any error, we thank Jeffrey Wooldridge, Achim Zeileis and three anonymous referees for useful comments. We also acknowledge kind editing assistance by Lisa Benedetti.</p>
</div>
<div id="bibliography" class="section level1 unnumbered">
<h1 class="unnumbered">Bibliography</h1>
<p><!-- Local IspellDict: english --> <!-- Local IspellPersDict: ~/emacs/.ispell-english --></p>
<div id="refs" class="references hanging-indent">
<div id="ref-AHRE:PINC:81">
<p>Ahrens, H., and R. Pincus. 1981. “On Two Measures of Unbalancedness in a One-Way Model and Their Relation to Efficiency.” <em>Biometrical Journal</em> 23 (3): 227–35. <a href="https://doi.org/10.1002/bimj.4710230302">https://doi.org/10.1002/bimj.4710230302</a>.</p>
</div>
<div id="ref-AMEM:71">
<p>Amemiya, T. 1971. “The Estimation of the Variances in a Variance–Components Model.” <em>International Economic Review</em> 12: 1–13.</p>
</div>
<div id="ref-AMEM:MACU:86">
<p>Amemiya, Takeshi, and Thomas E MaCurdy. 1986. “Instrumental-Variable Estimation of an Error-Components Model.” <em>Econometrica</em> 54 (4): 869–80.</p>
</div>
<div id="ref-ANDE:HSIA:81">
<p>Anderson, T. W., and C. Hsiao. 1981. “Estimation of Dynamic Models with Error Components.” <em>Journal of the American Statistical Association</em> 76: 598–606.</p>
</div>
<div id="ref-AREL:87">
<p>Arellano, Manuel. 1987. “Computing Robust Standard Errors for Within-Groups Estimators.” <em>Oxford Bulletin of Economics and Statistics</em> 49 (4): 431–34.</p>
</div>
<div id="ref-AREL:BOND:91">
<p>Arellano, M., and S. Bond. 1991. “Some Tests of Specification for Panel Data : Monte Carlo Evidence and an Application to Employment Equations.” <em>Review of Economic Studies</em> 58: 277–97.</p>
</div>
<div id="ref-BALE:VARA:87">
<p>Balestra, P., and J. Varadharajan–Krishnakumar. 1987. “Full Information Estimations of a System of Simultaneous Equations with Error Components.” <em>Econometric Theory</em> 3: 223–46.</p>
</div>
<div id="ref-BALT:CHAN:LI:98">
<p>Baltagi, Badi, YA Chang, and Q Li. 1998. “Testing for Random Individual and Time Effects Using Unbalanced Panel Data.” <em>Advances in Econometrics</em> 13 (January): 1–20.</p>
</div>
<div id="ref-BALT:FENG:KAO:12">
<p>Baltagi, Badi H., Qu Feng, and Chihwa Kao. 2012. “A Lagrange Multiplier Test for Cross-Sectional Dependence in a Fixed Effects Panel Data Model.” <em>Journal of Econometrics</em> 170 (1): 164–77. <a href="https://www.sciencedirect.com/science/article/pii/S030440761200098X">https://www.sciencedirect.com/science/article/pii/S030440761200098X</a>.</p>
</div>
<div id="ref-BALT:WU:99">
<p>Baltagi, Badi H., and Ping X. Wu. 1999. “Unequally Spaced Panel Data Regressions with Ar(1) Disturbances.” <em>Econometric Theory</em> 15 (6): 814–23.</p>
</div>
<div id="ref-BALT:05">
<p>Baltagi, B. H. 2005. <em>Econometric Analysis of Panel Data</em>. 3rd ed. John Wiley; Sons ltd.</p>
</div>
<div id="ref-BALT:13">
<p>———. 2013. <em>Econometric Analysis of Panel Data</em>. 5th ed. John Wiley; Sons ltd.</p>
</div>
<div id="ref-BALT:81">
<p>Baltagi, B. H. 1981. “Simultaneous Equations with Error Components.” <em>Journal of Econometrics</em> 17: 21–49.</p>
</div>
<div id="ref-BALT:CHAN:LI:92">
<p>Baltagi, B. H., Y. J. Chang, and Q. Li. 1992. “Monte Carlo Results on Several New and Existing Tests for the Error Components Model.” <em>Journal of Econometrics</em> 54: 95–120.</p>
</div>
<div id="ref-BALT:LI:90">
<p>Baltagi, B. H., and Q. Li. 1990. “A Lagrange Multiplier Test for the Error Components Model with Incomplete Panels.” <em>Econometric Reviews</em> 9: 103–7.</p>
</div>
<div id="ref-BALT:LI:91">
<p>Baltagi, B., and Q. Li. 1991. “A Joint Test for Serial Correlation and Random Individual Effects.” <em>Statistics and Probability Letters</em> 11: 277–80.</p>
</div>
<div id="ref-BALT:LI:95">
<p>———. 1995. “Testing AR(1) Against MA(1) Disturbances in an Error Component Model.” <em>Journal of Econometrics</em> 68: 133–51.</p>
</div>
<div id="ref-BATE:04">
<p>Bates, Douglas. 2004. “Least Squares Calculations in R.” <em>R–News</em> 4 (1): 17–20.</p>
</div>
<div id="ref-BATE:07">
<p>———. 2007. <em>Lme4: Linear Mixed–Effects Models Using S4 Classes</em>. <a href="https://CRAN.R-project.org">https://CRAN.R-project.org</a>.</p>
</div>
<div id="ref-BATE:MAEC:2016">
<p>Bates, Douglas, and Martin Maechler. 2016. <em>Matrix: Sparse and Dense Matrix Classes and Methods</em>. <a href="https://CRAN.R-project.org/package=Matrix">https://CRAN.R-project.org/package=Matrix</a>.</p>
</div>
<div id="ref-BERA:SOSA:YOON:01">
<p>Bera, A. K., W. Sosa–Escudero, and M. Yoon. 2001. “Tests for the Error Component Model in the Presence of Local Misspecification.” <em>Journal of Econometrics</em> 101: 1–23.</p>
</div>
<div id="ref-BHAR:FRAN:NARE:82">
<p>Bhargava, A., L. Franzini, and W. Narendranathan. 1982. “Serial Correlation and the Fixed Effects Model.” <em>The Review of Economic Studies</em> 49 (4): 533–49.</p>
</div>
<div id="ref-BIVA:08">
<p>Bivand, Roger. 2008. <em>Spdep: Spatial Dependence: Weighting Schemes, Statistics and Models</em>.</p>
</div>
<div id="ref-BLUN:BOND:98">
<p>Blundell, R., and S. Bond. 1998. “Initital Conditions and Moment Restrictions in Dynamic Panel Data Models.” <em>Journal of Econometrics</em> 87: 115–43.</p>
</div>
<div id="ref-BREU:MIZO:SCHM:89">
<p>Breusch, Trevor S, Grayham E Mizon, and Peter Schmidt. 1989. “Efficient Estimation Using Panel Data.” <em>Econometrica</em> 57 (3): 695–700.</p>
</div>
<div id="ref-BREU:PAGA:80">
<p>Breusch, T. S., and A. R. Pagan. 1980. “The Lagrange Multiplier Test and Its Applications to Model Specification in Econometrics.” <em>Review of Economic Studies</em> 47: 239–53.</p>
</div>
<div id="ref-CORN:RUPE:88">
<p>Cornwell, C., and P. Rupert. 1988. “Efficient Estimation with Panel Data: An Empirical Comparison of Instrumental Variables Estimators.” <em>Journal of Applied Econometrics</em> 3: 149–55.</p>
</div>
<div id="ref-CRIB:04">
<p>Cribari–Neto, F. 2004. “Asymptotic Inference Under Heteroskedasticity of Unknown Form.” <em>Computational Statistics &amp; Data Analysis</em> 45: 215–33.</p>
</div>
<div id="ref-Croissant:Millo:2008">
<p>Croissant, Yves, and Giovanni Millo. 2008. “Panel Data Econometrics in R: The Plm Package.” <em>Journal of Statistical Software</em> 27 (2). <a href="https://www.jstatsoft.org/v27/i02/">https://www.jstatsoft.org/v27/i02/</a>.</p>
</div>
<div id="ref-DEHO:SARA:06">
<p>De Hoyos, R. E., and V. Sarafidis. 2006. “Testing for Cross–Sectional Dependence in Panel–Data Models.” <em>The Stata Journal</em> 6 (4): 482–96.</p>
</div>
<div id="ref-R:2008">
<p> Development Core Team. 2008. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.r-project.org/">https://www.r-project.org/</a>.</p>
</div>
<div id="ref-DRUK:03">
<p>Drukker, D. M. 2003. “Testing for Serial Correlation in Linear Panel–Data Models.” <em>The Stata Journal</em> 3 (2): 168–77.</p>
</div>
<div id="ref-FOX:02">
<p>Fox, John. 2002. <em>An R and S–Plus Companion to Applied Regression</em>. Sage.</p>
</div>
<div id="ref-FOX:2016">
<p>———. 2016. <em>Car: Companion to Applied Regression</em>. <a href="https://CRAN.R-project.org/package=car">https://CRAN.R-project.org/package=car</a>.</p>
</div>
<div id="ref-GOUR:HOLL:MONF:82">
<p>Gourieroux, C., A. Holly, and A. Monfort. 1982. “Likelihood Ratio Test, Wald Test, and Kuhn–Tucker Test in Linear Models with Inequality Constraints on the Regression Parameters.” <em>Econometrica</em> 50: 63–80.</p>
</div>
<div id="ref-GREE:03">
<p>Greene, W. H. 2003. <em>Econometric Analysis</em>. 5th ed. Prentice Hall.</p>
</div>
<div id="ref-HARR:RUBI:78">
<p>Harrison, D., and D. L. Rubinfeld. 1978. “Hedonic Housing Prices and the Demand for Clean Air.” <em>Journal of Environmental Economics and Management</em> 5: 81–102.</p>
</div>
<div id="ref-HAUS:78">
<p>Hausman, J. A. 1978. “Specification Tests in Econometrics.” <em>Econometrica</em> 46: 1251–71.</p>
</div>
<div id="ref-HAUS:TAYL:81">
<p>Hausman, J. A., and W. E. Taylor. 1981. “Panel Data and Unobservable Individual Effects.” <em>Econometrica</em> 49: 1377–98.</p>
</div>
<div id="ref-HOLT:NEWE:ROSE:88">
<p>Holtz–Eakin, D., W. Newey, and H. S. Rosen. 1988. “Estimating Vector Autoregressions with Panel Data.” <em>Econometrica</em> 56: 1371–95.</p>
</div>
<div id="ref-HOND:85">
<p>Honda, Y. 1985. “Testing the Error Components Model with Non–Normal Disturbances.” <em>Review of Economic Studies</em> 52: 681–90.</p>
</div>
<div id="ref-HOTH:ZEIL:FARE:CUMM:MILL:MITC:2015">
<p>Hothorn, T., A. Zeileis, R. W. Farebrother, C. Cummins, G. Millo, and D. Mitchell. 2015. <em>Lmtest: Testing Linear Regression Models</em>. <a href="https://CRAN.R-project.org/package=lmtest">https://CRAN.R-project.org/package=lmtest</a>.</p>
</div>
<div id="ref-KING:WU:97">
<p>King, M. L., and P. X. Wu. 1997. “Locally Optimal One–Sided Tests for Multiparameter Hypothese.” <em>Econometric Reviews</em> 33: 523–29.</p>
</div>
<div id="ref-KLEI:ZEIL:08">
<p>Kleiber, Christian, and Achim Zeileis. 2008. <em>Applied Econometrics with R</em>. New York: Springer-Verlag. <a href="https://CRAN.R-project.org/package=AER">https://CRAN.R-project.org/package=AER</a>.</p>
</div>
<div id="ref-KOEN:NG:2016">
<p>Koenker, Roger, and Pin Ng. 2016. <em>SparseM: Sparse Linear Algebra</em>. <a href="https://CRAN.R-project.org/package=SparseM">https://CRAN.R-project.org/package=SparseM</a>.</p>
</div>
<div id="ref-LAIR:WARE:82">
<p>Laird, N. M., and J. H. Ware. 1982. “Random–Effects Models for Longitudinal Data.” <em>Biometrics</em> 38: 963–74.</p>
</div>
<div id="ref-LUML:ZEIL:2015">
<p>Lumley, T., and A. Zeileis. 2015. <em>Sandwich: Robust Covariance Matrix Estimators</em>. <a href="https://CRAN.R-project.org/package=sandwich">https://CRAN.R-project.org/package=sandwich</a>.</p>
</div>
<div id="ref-MACK:WHIT:85">
<p>MacKinnon, J. G., and H. White. 1985. “Some Heteroskedasticity–Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.” <em>Journal of Econometrics</em> 29: 305–25.</p>
</div>
<div id="ref-MUND:78">
<p>Mundlak, Yair. 1978. “On the Pooling of Time Series and Cross Section Data.” <em>Econometrica</em> 46 (1): 69–85.</p>
</div>
<div id="ref-MUNN:90">
<p>Munnell, A. 1990. “Why Has Productivity Growth Declined? Productivity and Public Investment.” <em>New England Economic Review</em>, 3–22.</p>
</div>
<div id="ref-NERLO:71">
<p>Nerlove, M. 1971. “Further Evidence on the Estimation of Dynamic Economic Relations from a Time–Series of Cross–Sections.” <em>Econometrica</em> 39: 359–82.</p>
</div>
<div id="ref-PESA:04">
<p>Pesaran, M. H. 2004. “General Diagnostic Tests for Cross Section Dependence in Panels.”</p>
</div>
<div id="ref-PESA:15">
<p>Pesaran, M. Hashem. 2015. “Testing Weak Cross-Sectional Dependence in Large Panels.” <em>Econometric Reviews</em> 34 (6-10): 1089–1117. <a href="https://doi.org/10.1080/07474938.2014.956623">https://doi.org/10.1080/07474938.2014.956623</a>.</p>
</div>
<div id="ref-PINH:BATE:00">
<p>Pinheiro, J. C., and D. Bates. 2000. <em>Mixed–Effects Models in S and S-Plus</em>. Springer-Verlag.</p>
</div>
<div id="ref-PINH:BATE:DEBR:SARK:07">
<p>Pinheiro, Jose, Douglas Bates, Saikat DebRoy, and Deepayan Sarkar the  Core team. 2007. <em>Nlme: Linear and Nonlinear Mixed Effects Models</em>. <a href="https://CRAN.R-project.org">https://CRAN.R-project.org</a>.</p>
</div>
<div id="ref-STOC:WATS:08">
<p>Stock, James H., and Mark W. Watson. 2008. “Heteroskedasticity–Robust Standard Errors for Fixed Effects Panel Data Regression.” <em>Econometrica</em> 76 (1): 155–74.</p>
</div>
<div id="ref-SWAM:70">
<p>Swamy, P. A. V. B. 1970. “Efficient Inference in a Random Coefficient Regression Model.” <em>Econometrica</em> 38: 311–23.</p>
</div>
<div id="ref-SWAM:AROR:72">
<p>Swamy, P. A. V. B., and S. S Arora. 1972. “The Exact Finite Sample Properties of the Estimators of Coefficients in the Error Components Regression Models.” <em>Econometrica</em> 40: 261–75.</p>
</div>
<div id="ref-THER:14">
<p>Therneau, Terry. 2014. <em>Bdsmatrix: Routines for Block Diagonal Symmetric Matrices</em>. <a href="https://CRAN.R-project.org/package=bdsmatrix">https://CRAN.R-project.org/package=bdsmatrix</a>.</p>
</div>
<div id="ref-WALL:HUSS:69">
<p>Wallace, T. D., and A. Hussain. 1969. “The Use of Error Components Models in Combining Cross Section with Time Series Data.” <em>Econometrica</em> 37 (1): 55–72.</p>
</div>
<div id="ref-WHIT:84b">
<p>White, H. 1984. <em>Asymptotic Theory for Econometricians</em>. New York: Academic press.</p>
</div>
<div id="ref-WHIT:80">
<p>White, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” <em>Econometrica</em> 48 (4): 817–38.</p>
</div>
<div id="ref-WIND:05">
<p>Windmeijer, F. 2005. “A Finite Sample Correction for the Variance of Linear Efficient Two–Steps GMM Estimators.” <em>Journal of Econometrics</em> 126: 25–51.</p>
</div>
<div id="ref-WOOL:02">
<p>Wooldridge, J. M. 2002. <em>Econometric Analysis of Cross–Section and Panel Data</em>. MIT press.</p>
</div>
<div id="ref-WOOL:10">
<p>———. 2010. <em>Econometric Analysis of Cross–Section and Panel Data</em>. MIT press.</p>
</div>
<div id="ref-ZEIL:04">
<p>Zeileis, A. 2004. “Econometric Computing with HC and HAC Covariance Matrix Estimators.” <em>Journal of Statistical Software</em> 11 (10): 1–17. <a href="https://www.jstatsoft.org/v11/i10/">https://www.jstatsoft.org/v11/i10/</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Comprehensive treatments are to be found in many econometrics textbooks, e.g., <span class="citation">Baltagi (2005)</span>, <span class="citation">Baltagi (2013)</span> or <span class="citation">Wooldridge (2002)</span>, <span class="citation">Wooldridge (2010)</span>: the reader is referred to these, especially to the first 9 chapters of <span class="citation">Baltagi (2005)</span>, <span class="citation">Baltagi (2013)</span>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>For the sake of exposition we are considering only the individual effects case here. There may also be time effects, which is a symmetric case, or both of them, so that the error has three components: <span class="math inline">\(u_{it}=\mu_{i}+\lambda_{t}+\epsilon_{it}\)</span>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Although in most models the individual and time effects cases are symmetric, there are exceptions: estimating the <em>first-difference</em> model on time effects is meaningless because cross-sections do not generally have a natural ordering, so trying <code>effect = &quot;time&quot;</code> stops with an error message as does <code>effect = &quot;twoways&quot;</code> which is not defined for first-difference models.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>See packages <code>lmtest</code> (<span class="citation">Hothorn et al. (2015)</span>) and <code>car</code> (<span class="citation">Fox (2016)</span>).<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Moreover, <code>coeftest()</code> provides a compact way of looking at coefficient estimates and significance diagnostics.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>The “random effect” is better termed “general FGLS” model, as in fact it does not have a proper random effects structure, but we keep this terminology for general language consistency.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>NB: Oneway King-Wu (<code>&quot;kw&quot;</code>) statistics (<code>&quot;individual&quot;</code> and <code>&quot;time&quot;</code>) coincide with the respective Honda statistics (<code>&quot;honda&quot;</code>); however, the twoway statistics of <code>&quot;kw&quot;</code> and <code>&quot;honda&quot;</code> differ.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>The <code>&quot;bp&quot;</code> test for unbalanced panels was derived in <span class="citation">Baltagi and Li (1990)</span>, the <code>&quot;kw&quot;</code> test for unbalanced panels in <span class="citation">Baltagi, Chang, and Li (1998)</span>. The <code>&quot;ghm&quot;</code> test and the <code>&quot;kw&quot;</code> test were extended to two–way effects in <span class="citation">Baltagi, Chang, and Li (1992)</span>. For a concise overview of all these statistics see <span class="citation">Baltagi (2013)</span> Sec. 4.2, pp. 68–76 (for balanced panels) and Sec. 9.5, pp. 200–203 (for unbalanced panels).<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Here we treat fixed and random effects alike, as components of the error term, according with the modern approach in econometrics (see <span class="citation">Wooldridge (2002)</span>, <span class="citation">Wooldridge (2010)</span>).<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Neglecting time effects may also lead to serial correlation in residuals (as observed in <span class="citation">Wooldridge (2002)</span> 10.4.1).<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p><span class="math inline">\(LM_3\)</span> in <span class="citation">Baltagi and Li (1995)</span>.<a href="#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>Corresponding to <span class="math inline">\(RSO^*_{\mu}\)</span> in the original paper.<a href="#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Baltagi and Li derive a basically analogous T-asymptotic test for first-order serial correlation in a FE panel model as a Breusch-Godfrey LM test on within residuals (see <span class="citation">Baltagi and Li (1995)</span> par. 2.3 and formula 12). They also observe that the test on within residuals can be used for testing on the RE model, as “the within transformation [time-demeaning, in our terminology] wipes out the individual effects, whether fixed or random”. Generalizing the Durbin-Watson test to FE models by applying it to fixed effects residuals is documented in <span class="citation">Bhargava, Franzini, and Narendranathan (1982)</span>, a (modified) version for unbalanced and/or non-consecutive panels is implemented in <code>pbnftest</code> as is Baltagi-Wu’s LBI statistic (for both see <span class="citation">Baltagi and Wu (1999)</span>).<a href="#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>see subsection <a href="#robust">robust covariance matrix estimation</a>.<a href="#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>Here, <span class="math inline">\(e_{it}\)</span> for notational simplicity (and as in Wooldridge): equivalent to <span class="math inline">\(\Delta \epsilon_{it}\)</span> in the general notation of the paper.<a href="#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>This is the case, e.g., if in an unobserved effects model when XSD is due to an unobservable factor structure, with factors that are uncorrelated with the regressors. In this case the within or random estimators are still consistent, although inefficient (see <span class="citation">De Hoyos and Sarafidis (2006)</span>).<a href="#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>The unbalanced version of this statistic uses max(Tij) for T in the bias-correction term<a href="#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>This is also the only solution when the time dimension’s length is insufficient for estimating the heterogeneous model.<a href="#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>The very comprehensive package <code>spdep</code> for spatial dependence analysis (see <span class="citation">Bivand (2008)</span>) contains features for creating, lagging and manipulating <em>neighbour list</em> objects of class <code>nb</code>, that can be readily converted to and from proximity matrices by means of the <code>nb2mat</code> function. Higher orders of the <span class="math inline">\(CD(p)\)</span> test can be obtained by lagging the corresponding <code>nb</code>s through <code>nblag</code>.<a href="#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>See <span class="citation">White (1980)</span> and <span class="citation">White (1984)</span>.<a href="#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>The HC3 and HC4 weighting schemes are computationally expensive and may hit memory limits for <span class="math inline">\(nT\)</span> in the thousands, where on the other hand it makes little sense to apply small sample corrections.<a href="#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>For <code>coeftest</code> set <code>df = Inf</code> to have the coefficients’ tests be performed with standard normal distribution instead of t distribution as we deal with a random effects model here. For these types of models, the precise distribution of the coefficients estimates is unknown<a href="#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p>Joint zero-restriction testing still allows providing the <code>vcov</code> of the unrestricted model as a matrix, see the documentation of package <code>lmtest</code>.<a href="#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>This discussion does not consider GMM models. One of the basic reasons for econometricians not to choose maximum likelihood methods in estimation is that the strict exogeneity of regressors assumption required for consistency of the ML models reported in the following is often inappropriate in economic settings.<a href="#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>The standard reference on the subject of mixed models in <code>S</code>/<code>R</code> is <span class="citation">Pinheiro and Bates (2000)</span>.<a href="#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn26"><p>Lagrange Multiplier tests based on the likelihood principle are suitable for testing against more general alternatives on the basis of a maintained model with spherical residuals and find therefore application in testing for departures from the classical hypotheses on the error term. The seminal reference is <span class="citation">Breusch and Pagan (1980)</span>.<a href="#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>For fixed effects estimation, as the sample grows (on the dimension on which the fixed effects are specified) so does the number of parameters to be estimated. Estimation of individual fixed effects is <span class="math inline">\(T\)</span>– (but not <span class="math inline">\(n\)</span>–) consistent, and the opposite.<a href="#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>In doing so, we stress that “equivalence” concerns only the specification of the model, and neither the appropriateness nor the relative efficiency of the relevant estimation techniques, which will of course be dependent on the context. Unlike their mixed model counterparts, the specifications in <code>plm</code> are, strictly speaking, distribution-free. Nevertheless, for the sake of exposition, in the following we present them in the setting which ensures consistency and efficiency (e.g., we consider the hypothesis of spherical errors part of the specification of pooled OLS and so forth).<a href="#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p>Take heed that here, in contrast to the usual meaning of serial correlation in time series, we always speak of serial correlation <em>between the errors of each group</em>.<a href="#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>note that the time index is coerced to numeric before the estimation.<a href="#fnref30" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
